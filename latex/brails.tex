%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=2,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{User Manual}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{0}



\title{BRAILS}
\date{May 26, 2021}
\release{2.0}
\author{Charles Wang}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{brails-demo}.gif}


\chapter{What is BRAILS}
\label{\detokenize{index:what-is-brailsname}}
\sphinxAtStartPar
The SimCenter tool \sphinxhyphen{} Building Recognition using AI at Large\sphinxhyphen{}Scale (BRAILS) is an AI\sphinxhyphen{}enabled software to assist regional\sphinxhyphen{}scale simulations. BRAILS is a prototype development that utilizes machine learning (ML), deep learning (DL), and computer vision (CV) to extract information from satellite and street view images for being used in computational modeling and risk assessment of the built environment. It also provides the architecture, engineering, and construction professionals the insight and tools to more efficiently plan, design, construct, and manage buildings and infrastructure systems.

\sphinxAtStartPar
The released v2.0 is re\sphinxhyphen{}structured with modules for performing specific analyses of images.
The expanded module library enables BRAILS’ capability of predicting a broader spectrum of building attributes including occupancy class, roof type, foundation elevation, year built, soft\sphinxhyphen{}story.

\sphinxAtStartPar
The new release also features a streamlined workflow, CityBuilder, for automatic creation of regional\sphinxhyphen{}scale building inventories by fusing multiple sources of data, including OpenStreetMap, Microsoft Footprint Data, Google Maps, and extracting information from them using the modules.

\sphinxAtStartPar
Examples of BRAILS’ application in natural hazard engineering include:
The identification of roof shapes, occupancy type, number of stories, construction year, and foundation elevation to improve the damage and loss calculations for the hurricane workflow; The identification of soft\sphinxhyphen{}story buildings to improve models in earthquake workflows.


\section{Acknowledgments}
\label{\detokenize{common/user_manual/acknowledgments:acknowledgments}}\label{\detokenize{common/user_manual/acknowledgments:lblacknowledgements}}\label{\detokenize{common/user_manual/acknowledgments::doc}}
\sphinxAtStartPar
This material is based upon work supported by the National Science Foundation under Grant No. 1612843. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.


\section{About}
\label{\detokenize{common/user_manual/about:about}}\label{\detokenize{common/user_manual/about:lblabout}}\label{\detokenize{common/user_manual/about::doc}}
\sphinxAtStartPar
This is an open\sphinxhyphen{}source research application, \sphinxhref{https://github.com/NHERI-SimCenter/BRAILS}{BRAILS Github page},
released under a \sphinxstylestrong{BSD clause 2} license, \hyperref[\detokenize{common/license:lbllicense}]{Section \ref{\detokenize{common/license:lbllicense}}}.
\sphinxstyleemphasis{BRAILS} focuses on harvesting and analyzing regional building information based on data from multiple sources to
assist decision makings in various sections, such as urban plan, risk management, etc.

\sphinxAtStartPar
In SimCenter, one of the successful applications of \sphinxstyleemphasis{BRAILS} is in assisting
the assessment of natural hazards and their imparts on the built environment.
The increase in the intensity of natural hazards is magnifying the impact of such events on our society.
In order to quantify and mitigate the risk due to the hazards and to prepare for the potential impacts in a region,
it is necessary to analyze existing buildings that are pertinent to natural hazard analysis and risk management.
However, gathering the building information in a region\sphinxhyphen{} or city\sphinxhyphen{}scale and analyzing them is a laborious and expensive undertaking.

\sphinxAtStartPar
\sphinxstyleemphasis{BRAILS} provides a framework for building information generation/gathering to support regional hazard analysis.
In this framework, different types of data are acquired from multiple sources
and are fused to semantically profile each building in a region.
Specifically, deep learning technique is employed to analyze street or satellite images.
Pretrained convolutional neural networks shipped with \sphinxstyleemphasis{BRAILS} are capable of analyzing images and detecting building properties that indicate vulnerabilities to natural hazards.
A novel data mining tool is integrated to overcome the data scarcity issue, quantify the uncertainty and enrich the data repository.
With this framework, building inventories of cities can be analyzed and the results provide the insights for further disaster and risk management planning and simulations.


\section{Installation}
\label{\detokenize{common/user_manual/installation:installation}}\label{\detokenize{common/user_manual/installation:lbl-install}}\label{\detokenize{common/user_manual/installation::doc}}
\sphinxAtStartPar
BRAILS is developed in Python3.
You can install BRAILS using pip3 (you can use pip is your default pip is pip3):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pip3 install \PYGZhy{}U BRAILS
\end{sphinxVerbatim}

\sphinxAtStartPar
Windows users may experience difficulties because of two dependencies required: GDAL and Fiona.

\sphinxAtStartPar
If you have difficulties installing BRAILS using the above command, please check the {\hyperref[\detokenize{common/user_manual/troubleshooting:lbltroubleshooting}]{\sphinxcrossref{\DUrole{std,std-ref}{Troubleshooting}}}} section.


\section{User Guide}
\label{\detokenize{common/user_manual/userGuide:user-guide}}\label{\detokenize{common/user_manual/userGuide:lbl-tutorial}}\label{\detokenize{common/user_manual/userGuide::doc}}
\sphinxAtStartPar
BRAILS provides a streamlined workflow for creating building inventories.

\sphinxAtStartPar
The workflow calls functions from multiple modules, which are capable of extracting information from images.

\sphinxAtStartPar
Most modules can be used as standalone functions.


\subsection{1.x User Guide (Legacy)}
\label{\detokenize{common/user_manual/1.x:x-user-guide-legacy}}\label{\detokenize{common/user_manual/1.x:lbl-1-x-tutorial}}\label{\detokenize{common/user_manual/1.x::doc}}

\subsubsection{Prepare and fuse data}
\label{\detokenize{common/user_manual/preparedata:prepare-and-fuse-data}}\label{\detokenize{common/user_manual/preparedata:lbl-preparedata}}\label{\detokenize{common/user_manual/preparedata::doc}}

\paragraph{Get google map api key}
\label{\detokenize{common/user_manual/preparedata:get-google-map-api-key}}
\sphinxAtStartPar
Obtain the API key from \sphinxurl{https://developers.google.com/maps/documentation/embed/get-api-key}.

\sphinxAtStartPar
Define api key in src/keys.py file like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{GoogleMapAPIKey} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{replace this with your key}\PYG{l+s+s2}{\PYGZdq{}}
\end{sphinxVerbatim}


\paragraph{Prepare a list of building addresses in csv format}
\label{\detokenize{common/user_manual/preparedata:prepare-a-list-of-building-addresses-in-csv-format}}
\sphinxAtStartPar
For example, Atlantic\_Cities\_Addrs.csv looks like \hyperref[\detokenize{common/user_manual/preparedata:address-list}]{Fig.\@ \ref{\detokenize{common/user_manual/preparedata:address-list}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{addressList}.png}
\caption{Address list}\label{\detokenize{common/user_manual/preparedata:id2}}\label{\detokenize{common/user_manual/preparedata:address-list}}\end{figure}

\sphinxAtStartPar
Noticing that, in addition to addresses, there are extra columns containing basic building information (e.g., stories, occupancy, etc.).
These basic information are scraped from tax websites.
It should be noted that, for a portion of these buildings, some building information may be missing from the websites.
In this case, just leave them blank in csv file.

\sphinxAtStartPar
Once \sphinxhref{https://berkeley.box.com/shared/static/hi0nzfykbadtczioj4tymrsjjgwahhbw.csv}{Atlantic\_Cities\_Addrs.csv} is prepared, define the path of it in src/confiugre.py like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cleanedBIMFileName} \PYG{o}{=} \PYG{n}{dataDir}\PYG{o}{+}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/Atlantic\PYGZus{}Cities\PYGZus{}Addrs.csv}\PYG{l+s+s2}{\PYGZdq{}}
\end{sphinxVerbatim}


\paragraph{Prepare a boundary file of the region of interest in geojson format}
\label{\detokenize{common/user_manual/preparedata:prepare-a-boundary-file-of-the-region-of-interest-in-geojson-format}}
\sphinxAtStartPar
Define the path in src/configure.py like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{RegionBoundaryFileName} \PYG{o}{=} \PYG{n}{dataDir}\PYG{o}{+}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/AtlanticCoastalCities\PYGZus{}Boundary.geojson}\PYG{l+s+s2}{\PYGZdq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
For this demo, we have prepared this boundary file for you, download from \sphinxhref{https://berkeley.box.com/shared/static/gfridzvcjo6k533554q9plh0g3v9fpzi.geojson}{here}.


\paragraph{Prepare building footprints in geojson format}
\label{\detokenize{common/user_manual/preparedata:prepare-building-footprints-in-geojson-format}}
\sphinxAtStartPar
AI generated building footprints database \sphinxhyphen{}\textgreater{} \sphinxhref{https://github.com/microsoft/USBuildingFootprints}{USBuildingFootprints}.

\sphinxAtStartPar
We have prepared a cleaned version that just contains Atlantic coastal cities, download from \sphinxhref{https://berkeley.box.com/shared/static/0ueibjzbo1b0mgru4h6n8l2rmww8nx0z.geojson}{here}.

\sphinxAtStartPar
Define the path of this footprints file in src/configure.py like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{BuildingFootPrintsFileName} \PYG{o}{=} \PYG{n}{dataDir}\PYG{o}{+}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/AtlanticCoastalCities\PYGZus{}Footprints.geojson}\PYG{l+s+s2}{\PYGZdq{}}
\end{sphinxVerbatim}


\paragraph{Geocode buildings and create a basic BIM file for this region.}
\label{\detokenize{common/user_manual/preparedata:geocode-buildings-and-create-a-basic-bim-file-for-this-region}}
\sphinxAtStartPar
Define the file path to store BIM for all buildings in src/configure.py like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{resultBIMFileName} \PYG{o}{=} \PYG{n}{dataDir}\PYG{o}{+}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/Atlantic\PYGZus{}Cities\PYGZus{}BIM.geojson}\PYG{l+s+s2}{\PYGZdq{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
Then run the following command from src/preparedata (This will cost \$ because it calls Google API.
To avoid this, download the \sphinxhref{https://berkeley.box.com/shared/static/mb8dya89hslfj1eo8rzns2v6gllq4x68.zip}{Atlantic geocoding file} and unzip it in your data/preparedata dir.
The code will first look into this dir for geocoding information, if it was not there, the code will call Google API.)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
python geocoding\PYGZus{}addr.py
\end{sphinxVerbatim}

\sphinxAtStartPar
This will create a BIM file Atlantic\_Cities\_BIM.geojson containing basic building information within the interested region.
The generated BIM file can be visulized in a GIS software, such as QGIS.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZob{}
    \PYGZdq{}type\PYGZdq{}: \PYGZdq{}FeatureCollection\PYGZdq{},
    \PYGZdq{}features\PYGZdq{}: [\PYGZob{}
                    \PYGZdq{}type\PYGZdq{}: \PYGZdq{}Feature\PYGZdq{},
                    \PYGZdq{}id\PYGZdq{}: \PYGZdq{}8460\PYGZdq{},
                    \PYGZdq{}properties\PYGZdq{}: \PYGZob{}
                            \PYGZdq{}id\PYGZdq{}: \PYGZdq{}8460\PYGZdq{},
                            \PYGZdq{}lat\PYGZdq{}: 39.371879,
                            \PYGZdq{}lon\PYGZdq{}: \PYGZhy{}74.456126,
                            \PYGZdq{}address\PYGZdq{}: \PYGZdq{}1970 W RIVERSIDE DR, ATLANTIC CITY,NJ\PYGZdq{},
                            \PYGZdq{}stories\PYGZdq{}: 2,
                            \PYGZdq{}yearBuilt\PYGZdq{}: 2006,
                            \PYGZdq{}occupancy\PYGZdq{}: \PYGZdq{}Residential\PYGZdq{},
                            \PYGZdq{}structureType\PYGZdq{}: \PYGZdq{}Frame\PYGZdq{},
                            \PYGZdq{}buildingDescription\PYGZdq{}: \PYGZdq{}2SF\PYGZdq{},
                            \PYGZdq{}city\PYGZdq{}: \PYGZdq{}ATLANTIC CITY CITY\PYGZdq{}
                    \PYGZcb{},
                    \PYGZdq{}geometry\PYGZdq{}: \PYGZob{}
                            \PYGZdq{}type\PYGZdq{}: \PYGZdq{}Polygon\PYGZdq{},
                            \PYGZdq{}coordinates\PYGZdq{}: [
                                    [
                                            [\PYGZhy{}74.45606, 39.371837],
                                            [\PYGZhy{}74.455935, 39.371934],
                                            [\PYGZhy{}74.456037, 39.372013],
                                            [\PYGZhy{}74.456162, 39.371916],
                                            [\PYGZhy{}74.45606, 39.371837]
                                    ]
                            ]
                    \PYGZcb{}
            \PYGZcb{},
            \PYGZob{}
                    \PYGZdq{}type\PYGZdq{}: \PYGZdq{}Feature\PYGZdq{},
                    \PYGZdq{}id\PYGZdq{}: \PYGZdq{}8461\PYGZdq{},
                    \PYGZdq{}properties\PYGZdq{}: \PYGZob{}
                            \PYGZdq{}id\PYGZdq{}: \PYGZdq{}8461\PYGZdq{},
                            \PYGZdq{}lat\PYGZdq{}: 39.3716807,
                            \PYGZdq{}lon\PYGZdq{}: \PYGZhy{}74.4513949,
                            \PYGZdq{}address\PYGZdq{}: \PYGZdq{}1619 COLUMBIA AVE, ATLANTIC CITY,NJ\PYGZdq{},
                            \PYGZdq{}stories\PYGZdq{}: 2,
                            \PYGZdq{}yearBuilt\PYGZdq{}: 1979,
                            \PYGZdq{}occupancy\PYGZdq{}: \PYGZdq{}Residential\PYGZdq{},
                            \PYGZdq{}structureType\PYGZdq{}: \PYGZdq{}Frame\PYGZdq{},
                            \PYGZdq{}buildingDescription\PYGZdq{}: \PYGZdq{}2SF\PYGZdq{},
                            \PYGZdq{}city\PYGZdq{}: \PYGZdq{}ATLANTIC CITY CITY\PYGZdq{}
                    \PYGZcb{},
                    \PYGZdq{}geometry\PYGZdq{}: \PYGZob{}
                            \PYGZdq{}type\PYGZdq{}: \PYGZdq{}Polygon\PYGZdq{},
                            \PYGZdq{}coordinates\PYGZdq{}: [
                                    [
                                            [\PYGZhy{}74.451353, 39.371717],
                                            [\PYGZhy{}74.451493, 39.371755],
                                            [\PYGZhy{}74.451526, 39.37168],
                                            [\PYGZhy{}74.451386, 39.371643],
                                            [\PYGZhy{}74.451353, 39.371717]
                                    ]
                            ]
                    \PYGZcb{}
            \PYGZcb{}
    ]
\PYGZcb{}
\end{sphinxVerbatim}


\subsubsection{Train a model}
\label{\detokenize{common/user_manual/train:train-a-model}}\label{\detokenize{common/user_manual/train:lbl-train}}\label{\detokenize{common/user_manual/train::doc}}
\sphinxAtStartPar
There are pretrained ConvNets released with BRAILS that can be used out of the box.

\sphinxAtStartPar
However, if the user is interested in training his / her own ConvNets, the following is an example demonstrating
how to train a classifier (for roof shape), and how to use the trained model in the BRAILS application.


\paragraph{Train}
\label{\detokenize{common/user_manual/train:train}}
\sphinxAtStartPar
The image data set for training has been prepared by SimCenter and can be download from here:

\sphinxAtStartPar
Charles Wang. (2019). Random satellite images of buildings (Version v1.0) {[}Data set{]}. Zenodo. \sphinxurl{http://doi.org/10.5281/zenodo.3521067}.

\sphinxAtStartPar
To train on the downloaded data, run the following command in the terminal

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cd src/training/
python train\PYGZus{}classifier.py \PYGZhy{}\PYGZhy{}img\PYGZus{}dir \PYGZlt{}IMAGE\PYGZus{}DIRECTORY\PYGZgt{} \PYGZhy{}\PYGZhy{}model\PYGZus{}dir \PYGZlt{}MODEL\PYGZus{}DIRECTORY\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
IMAGE\_DIRECTORY is the directory where you have your images for training

\begin{sphinxVerbatim}[commandchars=\\\{\}]
IMAGE\PYGZus{}DIRECTORY
│── class\PYGZus{}1
│       └── *.png
│── class\PYGZus{}2
|      └── *.png
│── ...
|
└── class\PYGZus{}n
       └── *.png
\end{sphinxVerbatim}

\sphinxAtStartPar
MODEL\_DIRECTORY is the directory where the trained model will be saved.

\sphinxAtStartPar
It is better to run the above code on a GPU machine.


\paragraph{Predict}
\label{\detokenize{common/user_manual/train:predict}}
\sphinxAtStartPar
Now we use the trained model to predict roof types based on satellite images.

\sphinxAtStartPar
Firstly we need to download those images by calling Google API (will cost \$).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cd src/predicting
python downloadRoofImages.py
\end{sphinxVerbatim}

\sphinxAtStartPar
To save \$, instead of running the above command, you can just download them from
\sphinxhref{https://berkeley.box.com/shared/static/n8l9kusi9eszsnnkefq37fofz22680t2.zip}{here}.

\sphinxAtStartPar
Now predictions can be made using the trained model:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cd src/predicting
python predict.py \PYGZhy{}\PYGZhy{}image\PYGZus{}dir \PYGZlt{}IMAGE\PYGZus{}DIRECTORY\PYGZgt{} \PYGZhy{}\PYGZhy{}model\PYGZus{}path \PYGZlt{}MODEL\PYGZus{}PATH\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
IMAGE\_DIRECTORY is the directory of your images. MODEL\_PATH is the folder where you have your trained model saved.


\subsubsection{Make predictions}
\label{\detokenize{common/user_manual/predict:make-predictions}}\label{\detokenize{common/user_manual/predict:lbl-train}}\label{\detokenize{common/user_manual/predict::doc}}

\paragraph{Predict}
\label{\detokenize{common/user_manual/predict:predict}}
\sphinxAtStartPar
Now we can use the trained model to predict on new images.

\sphinxAtStartPar
We show a example here: predict roof type based on satellite images.

\sphinxAtStartPar
Firstly we need to download those satellite images by calling Google API (will cost \$).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cd src/predicting
python downloadRoofImages.py
\end{sphinxVerbatim}

\sphinxAtStartPar
To save \$, instead of running the above command, you can just get them from
\sphinxhref{https://berkeley.box.com/shared/static/n8l9kusi9eszsnnkefq37fofz22680t2.zip}{here}. that are prepared by SimCenter.

\sphinxAtStartPar
Predictions can now be made using the trained model:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cd src/predicting
python predict.py \PYGZhy{}\PYGZhy{}image\PYGZus{}dir \PYGZlt{}IMAGE\PYGZus{}DIRECTORY\PYGZgt{} \PYGZhy{}\PYGZhy{}model\PYGZus{}path \PYGZlt{}MODEL\PYGZus{}PATH\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
IMAGE\_DIRECTORY is the directory of your images. MODEL\_PATH is the folder where you have your trained model saved.


\subsubsection{Enhance the data}
\label{\detokenize{common/user_manual/enhance:enhance-the-data}}\label{\detokenize{common/user_manual/enhance:lbl-enhance}}\label{\detokenize{common/user_manual/enhance::doc}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{enhance}.gif}
\caption{Data enhancement}\label{\detokenize{common/user_manual/enhance:id1}}\end{figure}

\sphinxAtStartPar
To enhance the initial database, use \sphinxhref{https://github.com/charlesxwang/SURF}{SURF} to predict missing building information.

\sphinxAtStartPar
For more information about \sphinxstyleemphasis{SURF}, read its \sphinxhref{https://nheri-simcenter.github.io/SURF/}{documentation}.

\sphinxAtStartPar
To install \sphinxstyleemphasis{SURF}, run the following command in the terminal:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pip3 install pySURF
\end{sphinxVerbatim}

\sphinxAtStartPar
To enhance the initial database, run the following command in the terminal:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
python3 SURF.ET\PYGZhy{}AI.py
\PYGZsh{} the SURF.ET\PYGZhy{}AI.py file can be found in github
\PYGZsh{} https://github.com/charlesxwang/SURF
\end{sphinxVerbatim}

\sphinxAtStartPar
After the running of the enhance script above, a new geojson file containing BIMs for the city will be generated.
The missing values in the initial database are now filled with predicted values.

\sphinxAtStartPar
The following figures show the prediction errors for four building properties: year built, number of stories, structure type, occupancy.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{yearBuilt-prediction-error}.png}
\caption{Prediction error of year built}\label{\detokenize{common/user_manual/enhance:id2}}\end{figure}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{stories_Predictions_classification_error}.png}
\caption{Prediction error of number of stories}\label{\detokenize{common/user_manual/enhance:id3}}\end{figure}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{structureType_Predictions_classification_error}.png}
\caption{Prediction error of structure type}\label{\detokenize{common/user_manual/enhance:id4}}\end{figure}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{occupancy_Predictions_classification_error}.png}
\caption{Prediction error of occupancy}\label{\detokenize{common/user_manual/enhance:id5}}\end{figure}


\subsection{Data Preperation for Training and Inference}
\label{\detokenize{common/user_manual/dataPrep:data-preperation-for-training-and-inference}}\label{\detokenize{common/user_manual/dataPrep:lbl-dataprep}}\label{\detokenize{common/user_manual/dataPrep::doc}}

\subsubsection{Training Data}
\label{\detokenize{common/user_manual/dataPrep:training-data}}
\sphinxAtStartPar
The accuracy of a deep learning model is highly dependent on the dataset used to train it. If the training data contains minimal noise and includes a sufficient level of features necessary for model development, high model prediction accuracies are attainable. In image\sphinxhyphen{}based applications, data noise can be in many forms, such as incorrect image labels and imagery that lack the features sought by the model (e.g., occluded features, etc.).

\sphinxAtStartPar
In developing the pre\sphinxhyphen{}trained models in BRAILS, training images are subjected to rigorous prescreening to ensure sufficient visibility of target buildings in each image in the training set. Without this screening step, creating models with reasonable confidence levels becomes difficult, with the training accuracies being inversely proportional to the extent of noise. \hyperref[\detokenize{common/user_manual/dataPrep:noiseeffect}]{Table \ref{\detokenize{common/user_manual/dataPrep:noiseeffect}}} shows an example of one of our observations of this condition throughout SimCenter’s model development efforts. The confusion matrices in both figures are for models generated using identical model architecture and hyperparameters. The first confusion matrix in \hyperref[\detokenize{common/user_manual/dataPrep:noiseeffect}]{Table \ref{\detokenize{common/user_manual/dataPrep:noiseeffect}}} is for the model trained on a dataset of images that contained 20\% noisy data, while the second matrix is for the model trained on a  dataset of images that were fully prescreened before model training. After 100 epochs, the former model attained an F1\sphinxhyphen{}score of 47.43\%; the latter achieved an F1\sphinxhyphen{}score of 79.15\% for the same validation set.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Effect of high noise levels on training performance}\label{\detokenize{common/user_manual/dataPrep:id2}}\label{\detokenize{common/user_manual/dataPrep:noiseeffect}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{allDataNotCleaned}.png}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{allDataCleaned}.png}
\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{noisyImages}} shows a sample set of images removed after prescreening.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Sample noisy images removed by the prescreening algorithm}\label{\detokenize{common/user_manual/dataPrep:id3}}\label{\detokenize{common/user_manual/dataPrep:noisyimages}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{badImage1}.jpg}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{badImage2}.jpg}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{badImage3}.jpg}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{badImage4}.jpg}
\end{sphinxfigure-in-table}\relax
\\
\hline\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{badImage5}.jpg}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{badImage6}.jpg}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{badImage7}.jpg}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{badImage8}.jpg}
\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{cleanImages}} shows a sample set of images that were deemed suitable for model consumption by the prescreening algorithm.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Sample clean images retained by prescreening}\label{\detokenize{common/user_manual/dataPrep:id4}}\label{\detokenize{common/user_manual/dataPrep:id1}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{cleanImage1}.jpg}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{cleanImage2}.jpg}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{cleanImage3}.jpg}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{cleanImage4}.jpg}
\end{sphinxfigure-in-table}\relax
\\
\hline\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{cleanImage5}.jpg}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{cleanImage6}.jpg}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{cleanImage7}.jpg}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{cleanImage8}.jpg}
\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsubsection{Data Suitable for Inference}
\label{\detokenize{common/user_manual/dataPrep:data-suitable-for-inference}}

\subsection{Modules}
\label{\detokenize{common/user_manual/modules/modules:modules}}\label{\detokenize{common/user_manual/modules/modules:lbl-modules}}\label{\detokenize{common/user_manual/modules/modules::doc}}
\sphinxAtStartPar
The BRAILS framework consists of a series of modules.
The modules can be called and executed in a workflow. Each module can also be used as a standalone function.


\subsubsection{Generic Image Classifier}
\label{\detokenize{common/user_manual/modules/genericImageClassifier:generic-image-classifier}}\label{\detokenize{common/user_manual/modules/genericImageClassifier:lbl-genericimageclassifier}}\label{\detokenize{common/user_manual/modules/genericImageClassifier::doc}}
\sphinxAtStartPar
The Generic Image Classifier is a module that can be used for creating user defined classifier.

\sphinxAtStartPar
The user provides categorized images to this module.

\sphinxAtStartPar
An image classifier will be built automatically based on the images provided.

\sphinxAtStartPar
The classifier is then trained and saved locally.

\sphinxAtStartPar
The trained classifier can readily be used for inference readily and can be shared with other users.

\sphinxAtStartPar
During the inference stage, the classifier takes a list of images as the input, and predicts the classes of the images.

\sphinxAtStartPar
The following is an example, in which a classifier is created and trained.

\sphinxAtStartPar
The image dataset for this example contains street view images categorized according to construction materials.

\sphinxAtStartPar
The dataset can be downloaded \sphinxhref{https://zenodo.org/record/4416845/files/building\_materials.zip}{here}.

\sphinxAtStartPar
When unzipped, the file gives the ‘building\_materials’ which is a directory that contains the images for training:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
building\PYGZus{}materials
│── class\PYGZus{}1
│       └── *.png
│── class\PYGZus{}2
|      └── *.png
│── ...
|
└── class\PYGZus{}n
       └── *.png
\end{sphinxVerbatim}


\paragraph{Construct the image classifier}
\label{\detokenize{common/user_manual/modules/genericImageClassifier:construct-the-image-classifier}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} import the module
from brails.modules import ImageClassifier

\PYGZsh{} initialize the classifier, give it a name
materialClassifier = ImageClassifier(modelName=\PYGZsq{}materialClassifierV0.1\PYGZsq{})

\PYGZsh{} load data
materialClassifier.loadData(\PYGZsq{}building\PYGZus{}materials\PYGZsq{})
\end{sphinxVerbatim}


\paragraph{Train the model}
\label{\detokenize{common/user_manual/modules/genericImageClassifier:train-the-model}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} train the base model for 50 epochs and then fine tune for 200 epochs
materialClassifier.train(baseModel=\PYGZsq{}InceptionV3\PYGZsq{}, initial\PYGZus{}epochs=50,fine\PYGZus{}tune\PYGZus{}epochs=200)
\end{sphinxVerbatim}

\sphinxAtStartPar
It is recommended to run the above example on a GPU machine.

\sphinxAtStartPar
The following ML model training options are available for selection as the baseModel key:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsq{}Xception\PYGZsq{},
\PYGZsq{}VGG16\PYGZsq{},
\PYGZsq{}VGG19\PYGZsq{},
\PYGZsq{}ResNet50\PYGZsq{},
\PYGZsq{}ResNet101\PYGZsq{},
\PYGZsq{}ResNet152\PYGZsq{},
\PYGZsq{}ResNet50V2\PYGZsq{},
\PYGZsq{}ResNet101V2\PYGZsq{},
\PYGZsq{}ResNet152V2\PYGZsq{},
\PYGZsq{}InceptionV3\PYGZsq{},
\PYGZsq{}InceptionResNetV2\PYGZsq{},
\PYGZsq{}MobileNet\PYGZsq{},
\PYGZsq{}MobileNetV2\PYGZsq{},
\PYGZsq{}DenseNet121\PYGZsq{},
\PYGZsq{}DenseNet169\PYGZsq{},
\PYGZsq{}DenseNet201\PYGZsq{},
\PYGZsq{}NASNetMobile\PYGZsq{},
\PYGZsq{}NASNetLarge\PYGZsq{},
\PYGZsq{}EfficientNetB0\PYGZsq{},
\PYGZsq{}EfficientNetB1\PYGZsq{},
\PYGZsq{}EfficientNetB2\PYGZsq{},
\PYGZsq{}EfficientNetB3\PYGZsq{},
\PYGZsq{}EfficientNetB4\PYGZsq{},
\PYGZsq{}EfficientNetB5\PYGZsq{},
\PYGZsq{}EfficientNetB6\PYGZsq{},
\PYGZsq{}EfficientNetB7\PYGZsq{}
\end{sphinxVerbatim}


\paragraph{Use the model}
\label{\detokenize{common/user_manual/modules/genericImageClassifier:use-the-model}}
\sphinxAtStartPar
Now you can use the trained model to predict the (building materials) class for a given image.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} If you are running the inference from another place, you need to initialize the classifier firstly:
from brails.GenericImageClassifier import ImageClassifier
materialClassifier = ImageClassifier(modelName=\PYGZsq{}materialClassifierV0.1\PYGZsq{})

\PYGZsh{} define the paths of images in a list
imgs = [\PYGZsq{}building\PYGZus{}materials/concrete/469 VAN BUREN AVE Oakland2.jpg\PYGZsq{},
        \PYGZsq{}building\PYGZus{}materials/masonry/101 FAIRMOUNT AVE Oakland2.jpg\PYGZsq{},
        \PYGZsq{}building\PYGZus{}materials/wood/41 MOSS AVE Oakland2.jpg\PYGZsq{}]

\PYGZsh{} use the model to predict
predictions = materialClassifier.predict(imgs)
\end{sphinxVerbatim}

\sphinxAtStartPar
The predictions will be written in preds.csv under the current directory.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The generic image classifier is intended to illustrate the overall process of model training and prediction.
The classifier takes an image as the input and will always produce a prediction.
Since the classifier is trained to classify only a specific category of images, its prediction is meaningful only if
the input image belongs to the category the model is trained for.
\end{sphinxadmonition}


\subsubsection{Roof Shape Classifier}
\label{\detokenize{common/user_manual/modules/roofClassifier:roof-shape-classifier}}\label{\detokenize{common/user_manual/modules/roofClassifier:lbl-roofclassifier}}\label{\detokenize{common/user_manual/modules/roofClassifier::doc}}
\sphinxAtStartPar
The Roof Shape Classifier is a module built upon the {\hyperref[\detokenize{common/user_manual/modules/genericImageClassifier:lbl-genericimageclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{Generic Image Classifier}}}} module.

\sphinxAtStartPar
The module is shipped with BRAILS,
so you don’t have to install it standalone if you’ve installed BRAILS following the {\hyperref[\detokenize{common/user_manual/installation:lbl-install}]{\sphinxcrossref{\DUrole{std,std-ref}{Installation}}}} instruction.

\sphinxAtStartPar
It takes a list of satellite images of roof tops as the input, and classify the roof types into three categories: gabled, hipped, and flat.


\paragraph{Use the module}
\label{\detokenize{common/user_manual/modules/roofClassifier:use-the-module}}
\sphinxAtStartPar
A pretrained model is shipped with BRAILS. So you can use it directly without training your own model.

\sphinxAtStartPar
The first time you initialize this model, it will download the model from the internet to your local computer.

\sphinxAtStartPar
The images used in the example can be downloaded from \sphinxhref{https://zenodo.org/record/4562949/files/image\_examples.zip}{here}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} import the module
from brails.modules import RoofClassifier

\PYGZsh{} initialize a roof classifier
roofModel = RoofClassifier()

\PYGZsh{} define the paths of images in a list
imgs = [\PYGZsq{}image\PYGZus{}examples/Roof/gabled/76.png\PYGZsq{},
        \PYGZsq{}image\PYGZus{}examples/Roof/hipped/54.png\PYGZsq{},
        \PYGZsq{}image\PYGZus{}examples/Roof/flat/94.png\PYGZsq{}]

\PYGZsh{} use the model to predict
predictions = roofModel.predict(imgs)
\end{sphinxVerbatim}

\sphinxAtStartPar
The predictions look like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Image :  image\PYGZus{}examples/Roof/gabled/76.png     Class : gabled (83.21\PYGZpc{})
Image :  image\PYGZus{}examples/Roof/hipped/54.png     Class : hipped (100.0\PYGZpc{})
Image :  image\PYGZus{}examples/Roof/flat/94.png     Class : flat (97.68\PYGZpc{})
Results written in file roofType\PYGZus{}preds.csv
\end{sphinxVerbatim}

\sphinxAtStartPar
Sample images used in this example are:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{76}.png}
\sphinxfigcaption{image\_examples/Roof/gabled/76.png Gabled}\label{\detokenize{common/user_manual/modules/roofClassifier:id1}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{54}.png}
\sphinxfigcaption{image\_examples/Roof/hipped/54.png  Hipped}\label{\detokenize{common/user_manual/modules/roofClassifier:id2}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{94}.png}
\sphinxfigcaption{image\_examples/Roof/flat/94.png  Flat}\label{\detokenize{common/user_manual/modules/roofClassifier:id3}}\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The classifier takes an image as the input and will always produce a prediction.
Since the classifier is trained to classify only a specific category of images,
its prediction is meaningful only if the input image belongs to the category the model is trained for.
\end{sphinxadmonition}


\paragraph{Retrain the model}
\label{\detokenize{common/user_manual/modules/roofClassifier:retrain-the-model}}
\sphinxAtStartPar
You can retrain the existing model with your own data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} Load images from a folder
roofModel.loadData(\PYGZsq{}folder\PYGZhy{}of\PYGZhy{}images\PYGZsq{})

\PYGZsh{} Re\PYGZhy{}train it for only 1 epoch for this demo. You can increase it.
roofModel.retrain(initial\PYGZus{}epochs=1)

\PYGZsh{} Test the re\PYGZhy{}trained model
predictions = roofModel.predict(imgs)

\PYGZsh{} Save the re\PYGZhy{}trained model
roofModel.save(\PYGZsq{}myCoolNewModelv0.1\PYGZsq{})
\end{sphinxVerbatim}


\subsubsection{Occupancy Classifier}
\label{\detokenize{common/user_manual/modules/occupancyClassifier:occupancy-classifier}}\label{\detokenize{common/user_manual/modules/occupancyClassifier:lbl-occupancyclassifier}}\label{\detokenize{common/user_manual/modules/occupancyClassifier::doc}}
\sphinxAtStartPar
The Occupancy Classifier is a module built upon the {\hyperref[\detokenize{common/user_manual/modules/genericImageClassifier:lbl-genericimageclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{Generic Image Classifier}}}} module.

\sphinxAtStartPar
The module is shipped with BRAILS,
so you don’t have to install it standalone if you’ve installed BRAILS following the {\hyperref[\detokenize{common/user_manual/installation:lbl-install}]{\sphinxcrossref{\DUrole{std,std-ref}{Installation}}}} instruction.

\sphinxAtStartPar
It takes a list of street view images of residential buildings as the input, and classify the buildings into three categories:
RES1 (single family building), RES3 (multi\sphinxhyphen{}family building), COM(Commercial building).


\paragraph{Use the module}
\label{\detokenize{common/user_manual/modules/occupancyClassifier:use-the-module}}
\sphinxAtStartPar
A pretrained model is shipped with BRAILS. So you can use it directly without training your own model.

\sphinxAtStartPar
The first time you initialize this model, it will download the model from the internet to your local computer.

\sphinxAtStartPar
The images used in the example can be downloaded from \sphinxhref{https://zenodo.org/record/4627958/files/image\_examples.zip}{here}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} import the module
from brails.modules import OccupancyClassifier

\PYGZsh{} initialize an occupancy classifier
occupancyModel = OccupancyClassifier()

\PYGZsh{} define the paths of images in a list
imgs = [\PYGZsq{}image\PYGZus{}examples/Occupancy/RES1/36887.jpg\PYGZsq{},
    \PYGZsq{}image\PYGZus{}examples/Occupancy/RES3/37902.jpg\PYGZsq{},
    \PYGZsq{}image\PYGZus{}examples/Occupancy/COM/42915.jpg\PYGZsq{}]

\PYGZsh{} use the model to predict
predictions = occupancyModel.predict(imgs)
\end{sphinxVerbatim}

\sphinxAtStartPar
The predictions look like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Image :  image\PYGZus{}examples/Occupancy/RES1/36887.jpg     Class : RES1 (100.0\PYGZpc{})
Image :  image\PYGZus{}examples/Occupancy/RES3/37902.jpg     Class : RES3 (100.0\PYGZpc{})
Image :  image\PYGZus{}examples/Occupancy/COM/42915.jpg     Class : COM (100.0\PYGZpc{})
Results written in file tmp/occupancy\PYGZus{}preds.csv
\end{sphinxVerbatim}

\sphinxAtStartPar
Sample images used in this example are:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{36887}.jpg}
\sphinxfigcaption{Predicted as Single\sphinxhyphen{}family Building}\label{\detokenize{common/user_manual/modules/occupancyClassifier:id1}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{37902}.jpg}
\sphinxfigcaption{Predicted as Multi\sphinxhyphen{}family Building}\label{\detokenize{common/user_manual/modules/occupancyClassifier:id2}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{42915}.jpg}
\sphinxfigcaption{Predicted as Commercial Building}\label{\detokenize{common/user_manual/modules/occupancyClassifier:id3}}\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The classifier takes an image as the input and will always produce a prediction.
Since the classifier is trained to classify only a specific category of images,
its prediction is meaningful only if the input image belongs to the category the model is trained for.
\end{sphinxadmonition}


\paragraph{Retrain the model}
\label{\detokenize{common/user_manual/modules/occupancyClassifier:retrain-the-model}}
\sphinxAtStartPar
You can retrain the existing model with your own data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} Load images from a folder
occupancyModel.loadData(\PYGZsq{}folder\PYGZhy{}of\PYGZhy{}images\PYGZsq{})

\PYGZsh{} Re\PYGZhy{}train it for only 1 epoch for this demo. You can increase it.
occupancyModel.retrain(initial\PYGZus{}epochs=1)

\PYGZsh{} Test the re\PYGZhy{}trained model
predictions = occupancyModel.predict(imgs)

\PYGZsh{} Save the re\PYGZhy{}trained model
occupancyModel.save(\PYGZsq{}myCoolNewModelv0.1\PYGZsq{})
\end{sphinxVerbatim}


\subsubsection{Soft\sphinxhyphen{}story Building Classifier}
\label{\detokenize{common/user_manual/modules/softstoryClassifier:soft-story-building-classifier}}\label{\detokenize{common/user_manual/modules/softstoryClassifier:lbl-softstoryclassifier}}\label{\detokenize{common/user_manual/modules/softstoryClassifier::doc}}
\sphinxAtStartPar
The Soft\sphinxhyphen{}story Building Classifier is a module built upon the {\hyperref[\detokenize{common/user_manual/modules/genericImageClassifier:lbl-genericimageclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{Generic Image Classifier}}}} module.

\sphinxAtStartPar
The module is shipped with BRAILS,
so you don’t have to install it standalone if you’ve installed BRAILS following the {\hyperref[\detokenize{common/user_manual/installation:lbl-install}]{\sphinxcrossref{\DUrole{std,std-ref}{Installation}}}} instruction.

\sphinxAtStartPar
It takes a list of street view images of buildings as the input, and classify the buildings into two categories: soft\sphinxhyphen{}story building and  other building.


\paragraph{Use the module}
\label{\detokenize{common/user_manual/modules/softstoryClassifier:use-the-module}}
\sphinxAtStartPar
A pretrained model is shipped with BRAILS. So you can use it directly without training your own model.

\sphinxAtStartPar
The first time you initialize this model, it will download the model from the internet to your local computer.

\sphinxAtStartPar
The images used in the example can be downloaded from \sphinxhref{https://zenodo.org/record/4562949/files/image\_examples.zip}{here}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} import the module
from brails.modules import SoftstoryClassifier

\PYGZsh{} initilize a soft\PYGZhy{}story classifier
ssModel = SoftstoryClassifier()

\PYGZsh{} define the paths of images in a list
imgs = [\PYGZsq{}image\PYGZus{}examples/Softstory/Others/3110.jpg\PYGZsq{},
        \PYGZsq{}image\PYGZus{}examples/Softstory/Softstory/901.jpg\PYGZsq{}]

\PYGZsh{} use the model to predict
predictions = ssModel.predict(imgs)
\end{sphinxVerbatim}

\sphinxAtStartPar
The predictions look like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Image :  image\PYGZus{}examples/Softstory/Others/3110.jpg     Class : others (96.13\PYGZpc{})
Image :  image\PYGZus{}examples/Softstory/Softstory/901.jpg     Class : softstory (96.31\PYGZpc{})
Results written in file softstory\PYGZus{}preds.csv
\end{sphinxVerbatim}

\sphinxAtStartPar
Sample images used in this example are:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{3110}.jpg}
\sphinxfigcaption{image\_examples/Softstory/Others/3110.jpg Non\sphinxhyphen{}Soft\sphinxhyphen{}story Building}\label{\detokenize{common/user_manual/modules/softstoryClassifier:id1}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{901}.jpg}
\sphinxfigcaption{image\_examples/Softstory/Softstory/901.jpg Soft\sphinxhyphen{}story Building}\label{\detokenize{common/user_manual/modules/softstoryClassifier:id2}}\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The classifier takes an image as the input and will always produce a prediction.
Since the classifier is trained to classify only a specific category of images,
its prediction is meaningful only if the input image belongs to the category the model is trained for.
\end{sphinxadmonition}


\paragraph{Retrain the model}
\label{\detokenize{common/user_manual/modules/softstoryClassifier:retrain-the-model}}
\sphinxAtStartPar
You can retrain the existing model with your own data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} Load images from a folder
ssModel.loadData(\PYGZsq{}folder\PYGZhy{}of\PYGZhy{}images\PYGZsq{})

\PYGZsh{} Re\PYGZhy{}train it for only 1 epoch for this demo. You can increase it.
ssModel.retrain(initial\PYGZus{}epochs=1)

\PYGZsh{} Test the re\PYGZhy{}trained model
predictions = ssModel.predict(imgs)

\PYGZsh{} Save the re\PYGZhy{}trained model
ssModel.save(\PYGZsq{}myCoolNewModelv0.1\PYGZsq{})
\end{sphinxVerbatim}


\subsubsection{Year Built Classification}
\label{\detokenize{common/user_manual/modules/yearClassifier:year-built-classification}}\label{\detokenize{common/user_manual/modules/yearClassifier:lbl-yearclassifier}}\label{\detokenize{common/user_manual/modules/yearClassifier::doc}}
\sphinxAtStartPar
With this module images of houses can be classifier into categories,
where each represents a range of years in which the house was built.
For classification, the path of a folder holding the images has to be supplied.
The result will be a comma separated value file in that folder,
listing the filenames and classification result.

\sphinxAtStartPar
The code is optimized to extract suitable features from Google Streetview
images to classify houses into decades. However, the code does not make
assumptions about the semantic meaning of the provided folders. They
have to be consistent between the training, validation and test folders but
can be categorized by decades, or short or longer than a decade, and any specified timespans.

\sphinxAtStartPar
The predictions fall in 6 categories:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{0} \PYG{p}{:} \PYG{n}{before} \PYG{l+m+mi}{1969}
\PYG{l+m+mi}{1} \PYG{p}{:} \PYG{l+m+mi}{1970}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1979}
\PYG{l+m+mi}{2} \PYG{p}{:} \PYG{l+m+mi}{1980}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1989}
\PYG{l+m+mi}{3} \PYG{p}{:} \PYG{l+m+mi}{1990}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1999}
\PYG{l+m+mi}{4} \PYG{p}{:} \PYG{l+m+mi}{2000}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2009}
\PYG{l+m+mi}{5} \PYG{p}{:} \PYG{n}{after} \PYG{l+m+mi}{2010}
\end{sphinxVerbatim}


\paragraph{Use the module}
\label{\detokenize{common/user_manual/modules/yearClassifier:use-the-module}}
\sphinxAtStartPar
A pretrained model is shipped with BRAILS. So you can use it directly without training your own model.

\sphinxAtStartPar
The first time you initialize this model, it will download the model from the internet to your local computer.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} import the module
from brails.modules import YearBuiltClassifier

\PYGZsh{} initialize a year classifier
model = YearBuiltClassifier()

\PYGZsh{} define the paths of images in a list
from glob import glob
imgs = glob(\PYGZsq{}image\PYGZus{}examples/Year/*/*.jpg\PYGZsq{})

\PYGZsh{} use the model to predict
predictions = model.predict(imgs)
\end{sphinxVerbatim}

\sphinxAtStartPar
The module is currently under active development and testing.
More details about the training, modification, improvement of this module can be found \sphinxhref{https://github.com/NHERI-SimCenter/BRAILS/tree/master/brails/modules/Year\_Built\_Classifier}{here}.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The classifier takes an image as the input and will always produce a prediction.
Since the classifier is trained to classify only a specific category of images,
its prediction is meaningful only if the input image belongs to the category the model is trained for.
\end{sphinxadmonition}


\subsubsection{Raised Foundation Classification}
\label{\detokenize{common/user_manual/modules/foundationElevationClassifier:raised-foundation-classification}}\label{\detokenize{common/user_manual/modules/foundationElevationClassifier:lbl-foundationelevationclassifier}}\label{\detokenize{common/user_manual/modules/foundationElevationClassifier::doc}}

\paragraph{What is Raised Foundation Classification}
\label{\detokenize{common/user_manual/modules/foundationElevationClassifier:what-is-raised-foundation-classification}}
\sphinxAtStartPar
The code in this package enables to see if a building is elevated on piles piers or posts (PPP).

\sphinxAtStartPar
For classification, the path of a folder holding the images has to be supplied. The result will be a comma separated value file in that folder, listing the filenames, classification (1: elevated, 0: not elevated), and the confidence of the prediction.


\paragraph{Use the module}
\label{\detokenize{common/user_manual/modules/foundationElevationClassifier:use-the-module}}
\sphinxAtStartPar
A pretrained model is shipped with BRAILS. So you can use it directly without training your own model.

\sphinxAtStartPar
The first time you initialize this model, it will download the model from the internet to your local computer.

\sphinxAtStartPar
The images used in the example can be downloaded from \sphinxhref{https://zenodo.org/record/4562949/files/image\_examples.zip}{here}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} import the module
from brails.modules import FoundationHeightClassifier

\PYGZsh{} initialize a roof classifier
model = FoundationHeightClassifier()

\PYGZsh{} define the paths of images in a list
from glob import glob
imgs = glob(\PYGZsq{}image\PYGZus{}examples/Foundation/*/*.jpg\PYGZsq{})

\PYGZsh{} use the model to predict
predictions = model.predict(imgs)
\end{sphinxVerbatim}

\sphinxAtStartPar
The predictions look like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Image :  a.jpg     Class : 1 (52.5\PYGZpc{})
Image :  b.jpg     Class : 1 (56.36\PYGZpc{})
Image :  c.jpg     Class : 0 (83.4\PYGZpc{})
Image :  d.jpg     Class : 0 (63.43\PYGZpc{})
Results written in file tmp/FoundationElevation.csv
\end{sphinxVerbatim}

\sphinxAtStartPar
The images used in this example are:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{a}.jpg}
\sphinxfigcaption{image\_examples/Foundation/Elevated/a.jpg Elevated}\label{\detokenize{common/user_manual/modules/foundationElevationClassifier:id2}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{b}.jpg}
\sphinxfigcaption{image\_examples/Foundation/Elevated/b.jpg  Elevated}\label{\detokenize{common/user_manual/modules/foundationElevationClassifier:id3}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{c}.jpg}
\sphinxfigcaption{image\_examples/Foundation/NotElevated/c.jpg   Not Elevated}\label{\detokenize{common/user_manual/modules/foundationElevationClassifier:id4}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{d}.jpg}
\sphinxfigcaption{image\_examples/Foundation/NotElevated/d.jpg   Not Elevated}\label{\detokenize{common/user_manual/modules/foundationElevationClassifier:id5}}\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
This module is currently under active development and testing.
Currently, for the data set used, classification reaches an F1\sphinxhyphen{}score of 72\% on a random test set that holds 20\% of the data.
Further optional code to improve the quality and speed of the classification is available.
More details about the training, modification, improvement of this module can be found \sphinxhref{https://github.com/NHERI-SimCenter/BRAILS/tree/master/brails/modules/Foundation\_Classification}{here}.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The classifier takes an image as the input and will always produce a prediction.
Since the classifier is trained to classify only a specific category of images,
its prediction is meaningful only if the input image belongs to the category the model is trained for.
\end{sphinxadmonition}


\subsubsection{Number of Floors Detector}
\label{\detokenize{common/user_manual/modules/nFloorDetector:number-of-floors-detector}}\label{\detokenize{common/user_manual/modules/nFloorDetector:lbl-nfloordetector}}\label{\detokenize{common/user_manual/modules/nFloorDetector::doc}}
\sphinxAtStartPar
The module is bundled with BRAILS, hence its use does not require a separate installation if BRAILS was installed following the {\hyperref[\detokenize{common/user_manual/installation:lbl-install}]{\sphinxcrossref{\DUrole{std,std-ref}{Installation}}}} instructions.

\sphinxAtStartPar
This module enables automated detection of number of floors in a building from image input. It takes the directory for an image or folder of images as input and writes the number of floor detections for each images into a CSV file.


\paragraph{Use the module}
\label{\detokenize{common/user_manual/modules/nFloorDetector:use-the-module}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} Import the module
from brails.modules import NFloorDetector

\PYGZsh{} Initialize the detector
nfloorDetector = NFloorDetector()

\PYGZsh{} Define the path of the images:
imDir = \PYGZdq{}datasets/test/\PYGZdq{}

\PYGZsh{} Detect the number of floors in each image inside imDir and write them in a
\PYGZsh{} CSV file. The prediction can be also assigned to DataFrame variable:
predictions = nfloorDetector.predict(imDir)

\PYGZsh{} Train a new detector using EfficientDet\PYGZhy{}D7 for 50 epochs
nfloorDetector.load\PYGZus{}train\PYGZus{}data(rootDir=\PYGZdq{}datasets/\PYGZdq{})
nfloorDetector.train(compCoeff=7,numEpochs=50)
\end{sphinxVerbatim}


\subsection{Workflow}
\label{\detokenize{common/user_manual/workflow:workflow}}\label{\detokenize{common/user_manual/workflow:lbl-workflow-tutorial}}\label{\detokenize{common/user_manual/workflow::doc}}
\sphinxAtStartPar
The workflow is designed to facilitate the creation of regional building inventories.

\sphinxAtStartPar
It is implemented in a class, CityBuilder.
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{brails}\PYG{n+nn}{.}\PYG{n+nn}{CityBuilder} \PYG{k+kn}{import} \PYG{n}{CityBuilder}

\PYG{n}{cityBuilder} \PYG{o}{=} \PYG{n}{CityBuilder}\PYG{p}{(}\PYG{n}{attributes}\PYG{p}{,}
                \PYG{n}{numBldg}\PYG{p}{,}
                \PYG{n}{random}\PYG{p}{,}
                \PYG{n}{bbox}\PYG{p}{,}
                \PYG{n}{place}\PYG{p}{,}
                \PYG{n}{footPrints}\PYG{p}{,}
                \PYG{n}{save}\PYG{p}{,}
                \PYG{n}{fileName}\PYG{p}{,}
                \PYG{n}{workDir}\PYG{p}{,}
                \PYG{n}{GoogleMapAPIKey}\PYG{p}{,}
                \PYG{n}{overwrite}\PYG{p}{,}
                \PYG{n}{reDownloadImgs}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}
\begin{quote}\begin{description}
\item[{attributes (list)}] \leavevmode
\sphinxAtStartPar
A list of building attributes, such as {[}‘{\hyperref[\detokenize{common/user_manual/modules/roofClassifier:lbl-roofclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{roofshape}}}}’, ‘{\hyperref[\detokenize{common/user_manual/modules/occupancyClassifier:lbl-occupancyclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{occupancy}}}}’, ‘{\hyperref[\detokenize{common/user_manual/modules/softstoryClassifier:lbl-softstoryclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{softstory}}}}’, ‘{\hyperref[\detokenize{common/user_manual/modules/foundationElevationClassifier:lbl-foundationelevationclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{elevated}}}}’, ‘{\hyperref[\detokenize{common/user_manual/modules/yearClassifier:lbl-yearclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{year}}}}’, ‘{\hyperref[\detokenize{common/user_manual/modules/nFloorDetector:lbl-nfloordetector}]{\sphinxcrossref{\DUrole{std,std-ref}{numstories}}}}’{]}, which are available in the current version.

\item[{numBldg (int)}] \leavevmode
\sphinxAtStartPar
Number of buildings to generate.

\item[{random (bool)}] \leavevmode
\sphinxAtStartPar
Randomly select numBldg buildings from the database if random is True.

\item[{bbox (list)}] \leavevmode
\sphinxAtStartPar
{[}north, west, south, east{]}, which are latitudes and longitudes of two corners that define a region of interest.

\item[{place (str)}] \leavevmode
\sphinxAtStartPar
The region of interest, e.g., ‘Berkeley, California’.

\item[{footPrints (str)}] \leavevmode
\sphinxAtStartPar
The footprint provide, choose from ‘OSM’ or ‘Microsoft’. The default value is ‘OSM’.

\item[{save (bool)}] \leavevmode
\sphinxAtStartPar
Save temporary files. Default value is True.

\item[{fileName (str)}] \leavevmode
\sphinxAtStartPar
Name of the generated BIM file. Default value will be generated if not provided by the user.

\item[{workDir (str)}] \leavevmode
\sphinxAtStartPar
Work directory where all files will be saved. Default value is ‘./tmp’

\item[{GoogleMapAPIKey (str)}] \leavevmode
\sphinxAtStartPar
Google API Key. Must be provided to use the workflow.

\item[{overwrite (bool)}] \leavevmode
\sphinxAtStartPar
Overwrite existing tmp files. Default value is False.

\item[{reDownloadImgs (bool)}] \leavevmode
\sphinxAtStartPar
Re\sphinxhyphen{}download even an image exists locally. Default value is False.

\end{description}\end{quote}


\subsubsection{Specify attributes to be collected}
\label{\detokenize{common/user_manual/workflow:specify-attributes-to-be-collected}}
\sphinxAtStartPar
Use the key ‘attributes’ to specify a list of attributes you intend to collect for each building.
Available ones in the current version include:
{[}‘roofshape’,
‘occupancy’,
‘softstory’,
‘elevated’,
‘year’,
‘numstories’{]}.
These attributes will be inferred from images using specific {\hyperref[\detokenize{common/user_manual/modules/modules:lbl-modules}]{\sphinxcrossref{\DUrole{std,std-ref}{modules}}}}.
\begin{itemize}
\item {} 
\sphinxAtStartPar
roofshape is the roof class, details can be found in {\hyperref[\detokenize{common/user_manual/modules/roofClassifier:lbl-roofclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{Roof Shape Classifier}}}}.

\item {} 
\sphinxAtStartPar
occupancy is the occupancy class, details can be found in {\hyperref[\detokenize{common/user_manual/modules/occupancyClassifier:lbl-occupancyclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{Occupancy Classifier}}}}.

\item {} 
\sphinxAtStartPar
softstory is the soft\sphinxhyphen{}story attribute, details can be found in {\hyperref[\detokenize{common/user_manual/modules/softstoryClassifier:lbl-softstoryclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{Soft\sphinxhyphen{}story Building Classifier}}}}.

\item {} 
\sphinxAtStartPar
elevated is the foundation elevation attribute, details can be found in {\hyperref[\detokenize{common/user_manual/modules/foundationElevationClassifier:lbl-foundationelevationclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{Raised Foundation Classification}}}}.

\item {} 
\sphinxAtStartPar
year is the year built, details can be found in {\hyperref[\detokenize{common/user_manual/modules/yearClassifier:lbl-yearclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{Year Built Classification}}}}.

\item {} 
\sphinxAtStartPar
numstories is the number of stories, details can be found in {\hyperref[\detokenize{common/user_manual/modules/nFloorDetector:lbl-nfloordetector}]{\sphinxcrossref{\DUrole{std,std-ref}{Number of Floors Detector}}}}.

\end{itemize}


\subsubsection{Limit the number of buildings to be collected}
\label{\detokenize{common/user_manual/workflow:limit-the-number-of-buildings-to-be-collected}}\label{\detokenize{common/user_manual/workflow:limitthenumber}}
\sphinxAtStartPar
The workflow will download a street view image and a satellite view image for each building.
The images are downloaded from Google Maps using  your personal \sphinxhref{https://developers.google.com/maps/documentation/embed/get-api-key}{Google API Keys}.
The price of the API calls can be found \sphinxhref{https://cloud.google.com/maps-platform/pricing}{here}.
As of February 3, 2021, \$7 per 1,000 street view images and \$2 per 1,000 satellite images.
Each Google account has \$200 free monthly usage. Exceeding that limit will result in being charged by Google.

\sphinxAtStartPar
You can use the key ‘numBldg’ to limit the number of buildings to be generated.


\subsubsection{Control the selection randomness}
\label{\detokenize{common/user_manual/workflow:control-the-selection-randomness}}
\sphinxAtStartPar
In a region, numBldg of buildings will be generated.
You can use the key ‘random’ to specify if you want to randomly select numBldg buildings from the database.
If its value is False, BIM will be generated for the first numBldg buildings found in the footprint database.


\subsubsection{Define the region of interest}
\label{\detokenize{common/user_manual/workflow:define-the-region-of-interest}}
\sphinxAtStartPar
There are two options to define the region of interest: ‘bbox’ or ‘place’.

\sphinxAtStartPar
If ‘bbox’ is provided, the workflow will retrieve numBldg buildings within the bounding box defined by ‘bbox’.

\sphinxAtStartPar
If ‘bbox’ is empty and ‘place’ is provided, the workflow will search the database based on ‘place’.


\subsubsection{Footprints options}
\label{\detokenize{common/user_manual/workflow:footprints-options}}
\sphinxAtStartPar
Use the key ‘footPrints’ to specify the source of building footprints to be used in the workflow.
Currently, the workflow supports ‘
\sphinxhref{https://www.openstreetmap.org/}{OSM}’ and ‘\sphinxhref{https://github.com/microsoft/USBuildingFootprints}{Microsoft}’.


\subsubsection{Examples}
\label{\detokenize{common/user_manual/workflow:examples}}
\sphinxAtStartPar
Check the {\hyperref[\detokenize{common/user_manual/examples:lbl-examples}]{\sphinxcrossref{\DUrole{std,std-ref}{Examples}}}}.


\section{Troubleshooting}
\label{\detokenize{common/user_manual/troubleshooting:troubleshooting}}\label{\detokenize{common/user_manual/troubleshooting:lbltroubleshooting}}\label{\detokenize{common/user_manual/troubleshooting::doc}}

\subsection{Installation}
\label{\detokenize{common/user_manual/troubleshooting:installation}}
\sphinxAtStartPar
Windows users may experience difficulties because of two dependencies required: GDAL and Fiona.

\sphinxAtStartPar
If you are a Conda user and you are installing BRAILS in the Conda environment, this is unlikely to happen.

\sphinxAtStartPar
If you are not a Conda user, the solution is to download Fiona and GDAL’s wheel
files (\sphinxurl{https://www.lfd.uci.edu/~gohlke/pythonlibs/}) and manually install them.

\sphinxAtStartPar
Make sure you choose the correct files based on your Python version.

\sphinxAtStartPar
For example, if you are using Python3.8:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Go to \sphinxurl{https://www.lfd.uci.edu/~gohlke/pythonlibs/} and download these files:
\begin{quote}

\sphinxAtStartPar
GDAL\sphinxhyphen{}3.1.4\sphinxhyphen{}cp38\sphinxhyphen{}cp38\sphinxhyphen{}win\_amd64.whl

\sphinxAtStartPar
Fiona\sphinxhyphen{}1.8.18\sphinxhyphen{}cp38\sphinxhyphen{}cp38\sphinxhyphen{}win\_amd64.whl
\end{quote}

\end{enumerate}
\begin{quote}

\sphinxAtStartPar
here cp38 means python3.8.
\end{quote}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
pip install GDAL\sphinxhyphen{}3.1.4\sphinxhyphen{}cp38\sphinxhyphen{}cp38\sphinxhyphen{}win\_amd64.whl

\item {} 
\sphinxAtStartPar
pip install Fiona\sphinxhyphen{}1.8.18\sphinxhyphen{}cp38\sphinxhyphen{}cp38\sphinxhyphen{}win\_amd64.whl

\end{enumerate}

\sphinxAtStartPar
Then you’ll be able to:

\sphinxAtStartPar
pip3 install BRAILS


\subsection{Internet connection}
\label{\detokenize{common/user_manual/troubleshooting:internet-connection}}
\sphinxAtStartPar
The trained models and accompanying datasets, when called the first time, need to be downloaded from the internet.

\sphinxAtStartPar
Images also need to be downloaded during the running.

\sphinxAtStartPar
Therefore, please make sure you are connected to the internet.


\section{Examples}
\label{\detokenize{common/user_manual/examples:examples}}\label{\detokenize{common/user_manual/examples:lbl-examples}}\label{\detokenize{common/user_manual/examples::doc}}

\subsection{Example 1: Modules}
\label{\detokenize{common/user_manual/examples:example-1-modules}}
\sphinxAtStartPar
The following is an example showing how to call pretrained models to predict on images.

\sphinxAtStartPar
You can run this example on your computer or in this notebook on \sphinxhref{https://colab.research.google.com/drive/1zspDwK-rGA1gYcHZDnrQr\_3Z27JL-ooS?usp=sharing}{Google Colab}.

\sphinxAtStartPar
The images used in the example can be downloaded from here: \sphinxhref{https://zenodo.org/record/4095668/files/image\_examples.zip}{image\_examples.zip}.
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} import modules}
\PYG{k+kn}{from} \PYG{n+nn}{brails}\PYG{n+nn}{.}\PYG{n+nn}{modules} \PYG{k+kn}{import} \PYG{n}{RoofClassifier}\PYG{p}{,} \PYG{n}{OccupancyClassifier}\PYG{p}{,} \PYG{n}{SoftstoryClassifier}

\PYG{c+c1}{\PYGZsh{} initilize a roof classifier}
\PYG{n}{roofModel} \PYG{o}{=} \PYG{n}{RoofClassifier}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} initilize an occupancy classifier}
\PYG{n}{occupancyModel} \PYG{o}{=} \PYG{n}{OccupancyClassifier}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} initilize a soft\PYGZhy{}story classifier}
\PYG{n}{ssModel} \PYG{o}{=} \PYG{n}{SoftstoryClassifier}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} use the roof classifier}

\PYG{n}{imgs} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{image\PYGZus{}examples/Roof/gabled/76.png}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{image\PYGZus{}examples/Roof/hipped/54.png}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{image\PYGZus{}examples/Roof/flat/94.png}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{predictions} \PYG{o}{=} \PYG{n}{roofModel}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{imgs}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} use the occupancy classifier}

\PYG{n}{imgs} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{image\PYGZus{}examples/Occupancy/RES1/51563.png}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{image\PYGZus{}examples/Occupancy/RES3/65883.png}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{predictions} \PYG{o}{=} \PYG{n}{occupancyModel}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{imgs}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} use the softstory classifier}

\PYG{n}{imgs} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{image\PYGZus{}examples/Softstory/Others/3110.jpg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{image\PYGZus{}examples/Softstory/Softstory/901.jpg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{predictions} \PYG{o}{=} \PYG{n}{ssModel}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{imgs}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}


\subsection{Example 2: Workflow}
\label{\detokenize{common/user_manual/examples:example-2-workflow}}
\sphinxAtStartPar
The following is an example showing how to create a building inventory for a city.

\sphinxAtStartPar
You can run this example on your computer or in this notebook on \sphinxhref{https://colab.research.google.com/drive/1tG6xVRCmDyi6K8TWgoNd\_31vV034VcSO?usp=sharing}{Google Colab}.

\sphinxAtStartPar
You need to provide the Google maps API key for downloading street view and satellite images.

\sphinxAtStartPar
Instructions on obtaining the API key can be found here: \sphinxurl{https://developers.google.com/maps/documentation/embed/get-api-key}.

\sphinxAtStartPar
Use should limit the number of buildings (numBldg) because of {\hyperref[\detokenize{common/user_manual/workflow:limitthenumber}]{\sphinxcrossref{\DUrole{std,std-ref}{this}}}}.
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{brails}\PYG{n+nn}{.}\PYG{n+nn}{CityBuilder} \PYG{k+kn}{import} \PYG{n}{CityBuilder}

\PYG{n}{cityBuilder} \PYG{o}{=} \PYG{n}{CityBuilder}\PYG{p}{(}\PYG{n}{attributes}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{occupancy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{roofshape}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
               \PYG{n}{numBldg}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,}\PYG{n}{random}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{place}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Lake Charles, LA}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
               \PYG{n}{GoogleMapAPIKey}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{put\PYGZhy{}your\PYGZhy{}API\PYGZhy{}key\PYGZhy{}here}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
               \PYG{n}{overwrite}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{n}{BIM} \PYG{o}{=} \PYG{n}{cityBuilder}\PYG{o}{.}\PYG{n}{build}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}
\begin{quote}\begin{description}
\item[{attributes (list)}] \leavevmode
\sphinxAtStartPar
A list of building attributes, such as {[}‘story’, ‘occupancy’, ‘roofshape’{]}, which are available in the current version.

\item[{numBldg (int)}] \leavevmode
\sphinxAtStartPar
Number of buildings to generate.

\item[{random (bool)}] \leavevmode
\sphinxAtStartPar
Randomly select numBldg buildings from the database if random is True.

\item[{place (str)}] \leavevmode
\sphinxAtStartPar
The region of interest, e.g., Berkeley, California.

\item[{GoogleMapAPIKey (str)}] \leavevmode
\sphinxAtStartPar
Google API Key.

\item[{overwrite (bool)}] \leavevmode
\sphinxAtStartPar
Overwrite existing tmp files. Default value is False.

\end{description}\end{quote}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{Berkeley}.png}
\caption{Generated Buildings}\label{\detokenize{common/user_manual/examples:num-building-city}}\end{figure}


\subsection{Example 3: Workflow}
\label{\detokenize{common/user_manual/examples:example-3-workflow}}
\sphinxAtStartPar
The following is an example showing how to create a building inventory for a region defined using a bounding box.

\sphinxAtStartPar
You can run this example on your computer or in this notebook on \sphinxhref{https://colab.research.google.com/drive/1tG6xVRCmDyi6K8TWgoNd\_31vV034VcSO?usp=sharing}{Google Colab}.

\sphinxAtStartPar
You need to provide the Google maps API key for downloading street view and satellite images.

\sphinxAtStartPar
Instructions on obtaining the API key can be found here: \sphinxurl{https://developers.google.com/maps/documentation/embed/get-api-key}.

\sphinxAtStartPar
Use should limit the number of buildings (numBldg) because of {\hyperref[\detokenize{common/user_manual/workflow:limitthenumber}]{\sphinxcrossref{\DUrole{std,std-ref}{this}}}}.
\begin{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{brails}\PYG{n+nn}{.}\PYG{n+nn}{CityBuilder} \PYG{k+kn}{import} \PYG{n}{CityBuilder}

\PYG{n}{cityBuilder} \PYG{o}{=} \PYG{n}{CityBuilder}\PYG{p}{(}\PYG{n}{attributes}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{softstory}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{occupancy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{roofshape}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
               \PYG{n}{numBldg}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{random}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{bbox}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{37.872187}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{122.282178}\PYG{p}{,}\PYG{l+m+mf}{37.870629}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{122.279765}\PYG{p}{]}\PYG{p}{,}
               \PYG{n}{GoogleMapAPIKey}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{put\PYGZhy{}your\PYGZhy{}API\PYGZhy{}key\PYGZhy{}here}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
               \PYG{n}{overwrite}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{n}{BIM} \PYG{o}{=} \PYG{n}{cityBuilder}\PYG{o}{.}\PYG{n}{build}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
\end{quote}
\begin{quote}\begin{description}
\item[{attributes (list)}] \leavevmode
\sphinxAtStartPar
A list of building attributes, such as {[}‘story’, ‘occupancy’, ‘roofshape’{]}, which are available in the current version.

\item[{numBldg (int)}] \leavevmode
\sphinxAtStartPar
Number of buildings to generate.

\item[{random (bool)}] \leavevmode
\sphinxAtStartPar
Randomly select numBldg buildings from the database if random is True.

\item[{bbox (list)}] \leavevmode
\sphinxAtStartPar
{[}north, west, south, east{]}, which defines a region of interest.

\item[{GoogleMapAPIKey (str)}] \leavevmode
\sphinxAtStartPar
Google API Key.

\item[{overwrite (bool)}] \leavevmode
\sphinxAtStartPar
Overwrite existing tmp files. Default value is False.

\end{description}\end{quote}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{Christchurch}.png}
\caption{Generated Buildings}\label{\detokenize{common/user_manual/examples:id4}}\end{figure}


\section{Bugs \& Feauture Requests}
\label{\detokenize{common/user_manual/bugs:bugs-feauture-requests}}\label{\detokenize{common/user_manual/bugs:lblbugs}}\label{\detokenize{common/user_manual/bugs::doc}}
\sphinxAtStartPar
If you have any bugs, feauture requests, or questions about how to
install or use the application please post on the \sphinxhref{https://simcenter-messageboard.designsafe-ci.org/smf/index.php?board=10.0}{Message board}.
To avoid duplication try the \sphinxtitleref{Search} feature before creating a \sphinxtitleref{New Topic}.
When creating a \sphinxtitleref{New Topic} to report a bug or new feature request it would be helpful
if you could place in the \sphinxtitleref{Subject} area, as shown in \hyperref[\detokenize{common/user_manual/bugs:figbugreport}]{Fig.\@ \ref{\detokenize{common/user_manual/bugs:figbugreport}}}, an indication of what the post is about:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{BUG}, for bugs, i.e. BUG: Program crashes when starting

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{FEATURE}, for new feature requests, i.e. FEATURE: Incorporate OpenSeesPy

\end{enumerate}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{bugReport}.png}
\caption{Example of Submitting a Bug Report to Message Board}\label{\detokenize{common/user_manual/bugs:id1}}\label{\detokenize{common/user_manual/bugs:figbugreport}}\end{figure}


\section{Copyright and License}
\label{\detokenize{common/license:copyright-and-license}}\label{\detokenize{common/license:lbllicense}}\label{\detokenize{common/license::doc}}
\sphinxAtStartPar
BRAILS is copyright “The Regents of the University of California” and is licensed under the following BSD license:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{The} \PYG{n}{source} \PYG{n}{code} \PYG{n}{is} \PYG{n}{licensed} \PYG{n}{under} \PYG{n}{a} \PYG{n}{BSD} \PYG{l+m+mi}{2}\PYG{o}{\PYGZhy{}}\PYG{n}{Clause} \PYG{n}{License}\PYG{p}{.}

\PYG{n}{Copyright} \PYG{p}{(}\PYG{n}{c}\PYG{p}{)} \PYG{l+m+mi}{2017}\PYG{l+m+mi}{\PYGZhy{}2021}\PYG{p}{,} \PYG{n}{The} \PYG{n}{Regents} \PYG{n}{of} \PYG{n}{the} \PYG{n}{University} \PYG{n}{of} \PYG{n}{California}

\PYG{n}{All} \PYG{n}{rights} \PYG{n}{reserved}\PYG{p}{.}

\PYG{n}{Redistribution} \PYG{n}{and} \PYG{n}{use} \PYG{n}{in} \PYG{n}{source} \PYG{n}{and} \PYG{n}{binary} \PYG{n}{forms}\PYG{p}{,} \PYG{n}{with} \PYG{n}{or} \PYG{n}{without}
\PYG{n}{modification}\PYG{p}{,} \PYG{n}{are} \PYG{n}{permitted} \PYG{n}{provided} \PYG{n}{that} \PYG{n}{the} \PYG{n}{following} \PYG{n}{conditions} \PYG{n}{are} \PYG{n+nl}{met}\PYG{p}{:}

\PYG{l+m+mf}{1.} \PYG{n}{Redistributions} \PYG{n}{of} \PYG{n}{source} \PYG{n}{code} \PYG{n}{must} \PYG{n}{retain} \PYG{n}{the} \PYG{n}{above} \PYG{n}{copyright} \PYG{n}{notice}\PYG{p}{,} \PYG{k}{this}
   \PYG{n}{list} \PYG{n}{of} \PYG{n}{conditions} \PYG{n}{and} \PYG{n}{the} \PYG{n}{following} \PYG{n}{disclaimer}\PYG{p}{.}

\PYG{l+m+mf}{2.} \PYG{n}{Redistributions} \PYG{n}{in} \PYG{n}{binary} \PYG{n}{form} \PYG{n}{must} \PYG{n}{reproduce} \PYG{n}{the} \PYG{n}{above} \PYG{n}{copyright} \PYG{n}{notice}\PYG{p}{,}
   \PYG{k}{this} \PYG{n}{list} \PYG{n}{of} \PYG{n}{conditions} \PYG{n}{and} \PYG{n}{the} \PYG{n}{following} \PYG{n}{disclaimer} \PYG{n}{in} \PYG{n}{the} \PYG{n}{documentation}
  \PYG{n}{and}\PYG{o}{/}\PYG{n}{or} \PYG{n}{other} \PYG{n}{materials} \PYG{n}{provided} \PYG{n}{with} \PYG{n}{the} \PYG{n}{distribution}\PYG{p}{.}

\PYG{n}{THIS} \PYG{n}{SOFTWARE} \PYG{n}{IS} \PYG{n}{PROVIDED} \PYG{n}{BY} \PYG{n}{THE} \PYG{n}{COPYRIGHT} \PYG{n}{HOLDERS} \PYG{n}{AND} \PYG{n}{CONTRIBUTORS} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{AS IS}\PYG{l+s}{\PYGZdq{}}
\PYG{n}{AND} \PYG{n}{ANY} \PYG{n}{EXPRESS} \PYG{n}{OR} \PYG{n}{IMPLIED} \PYG{n}{WARRANTIES}\PYG{p}{,} \PYG{n}{INCLUDING}\PYG{p}{,} \PYG{n}{BUT} \PYG{n}{NOT} \PYG{n}{LIMITED} \PYG{n}{TO}\PYG{p}{,} \PYG{n}{THE}
\PYG{n}{IMPLIED} \PYG{n}{WARRANTIES} \PYG{n}{OF} \PYG{n}{MERCHANTABILITY} \PYG{n}{AND} \PYG{n}{FITNESS} \PYG{n}{FOR} \PYG{n}{A} \PYG{n}{PARTICULAR} \PYG{n}{PURPOSE} \PYG{n}{ARE}
\PYG{n}{DISCLAIMED}\PYG{p}{.} \PYG{n}{IN} \PYG{n}{NO} \PYG{n}{EVENT} \PYG{n}{SHALL} \PYG{n}{THE} \PYG{n}{COPYRIGHT} \PYG{n}{HOLDER} \PYG{n}{OR} \PYG{n}{CONTRIBUTORS} \PYG{n}{BE} \PYG{n}{LIABLE}
\PYG{n}{FOR} \PYG{n}{ANY} \PYG{n}{DIRECT}\PYG{p}{,} \PYG{n}{INDIRECT}\PYG{p}{,} \PYG{n}{INCIDENTAL}\PYG{p}{,} \PYG{n}{SPECIAL}\PYG{p}{,} \PYG{n}{EXEMPLARY}\PYG{p}{,} \PYG{n}{OR} \PYG{n}{CONSEQUENTIAL}
\PYG{n}{DAMAGES} \PYG{p}{(}\PYG{n}{INCLUDING}\PYG{p}{,} \PYG{n}{BUT} \PYG{n}{NOT} \PYG{n}{LIMITED} \PYG{n}{TO}\PYG{p}{,} \PYG{n}{PROCUREMENT} \PYG{n}{OF} \PYG{n}{SUBSTITUTE} \PYG{n}{GOODS} \PYG{n}{OR}
\PYG{n}{SERVICES}\PYG{p}{;} \PYG{n}{LOSS} \PYG{n}{OF} \PYG{n}{USE}\PYG{p}{,} \PYG{n}{DATA}\PYG{p}{,} \PYG{n}{OR} \PYG{n}{PROFITS}\PYG{p}{;} \PYG{n}{OR} \PYG{n}{BUSINESS} \PYG{n}{INTERRUPTION}\PYG{p}{)} \PYG{n}{HOWEVER}
\PYG{n}{CAUSED} \PYG{n}{AND} \PYG{n}{ON} \PYG{n}{ANY} \PYG{n}{THEORY} \PYG{n}{OF} \PYG{n}{LIABILITY}\PYG{p}{,} \PYG{n}{WHETHER} \PYG{n}{IN} \PYG{n}{CONTRACT}\PYG{p}{,} \PYG{n}{STRICT} \PYG{n}{LIABILITY}\PYG{p}{,}
\PYG{n}{OR} \PYG{n}{TORT} \PYG{p}{(}\PYG{n}{INCLUDING} \PYG{n}{NEGLIGENCE} \PYG{n}{OR} \PYG{n}{OTHERWISE}\PYG{p}{)} \PYG{n}{ARISING} \PYG{n}{IN} \PYG{n}{ANY} \PYG{n}{WAY} \PYG{n}{OUT} \PYG{n}{OF} \PYG{n}{THE} \PYG{n}{USE}
\PYG{n}{OF} \PYG{n}{THIS} \PYG{n}{SOFTWARE}\PYG{p}{,} \PYG{n}{EVEN} \PYG{n}{IF} \PYG{n}{ADVISED} \PYG{n}{OF} \PYG{n}{THE} \PYG{n}{POSSIBILITY} \PYG{n}{OF} \PYG{n}{SUCH} \PYG{n}{DAMAGE}\PYG{p}{.}
\end{sphinxVerbatim}
\phantomsection\label{\detokenize{index:lbl-technical-manual}}

\section{Theory and Implementation}
\label{\detokenize{common/technical_manual/theory:theory-and-implementation}}\label{\detokenize{common/technical_manual/theory:lbl-theory}}\label{\detokenize{common/technical_manual/theory::doc}}

\subsection{Framework}
\label{\detokenize{common/technical_manual/framework:framework}}\label{\detokenize{common/technical_manual/framework:lbl-framework}}\label{\detokenize{common/technical_manual/framework::doc}}
\sphinxAtStartPar
This section presents a framework that works as an abstraction providing generic functionalities and can be selectively
changed by additional user\sphinxhyphen{}written codes with user\sphinxhyphen{}provided input data,
thus providing a region\sphinxhyphen{}specific building information harvesting tool.
The framework provides a standard way to create realistic building inventory databases and it is a universal, reusable environment that provides particular functionalities facilitating regional\sphinxhyphen{}scale building information modeling.

\sphinxAtStartPar
As shown in \hyperref[\detokenize{common/technical_manual/framework:brailspipeline}]{Fig.\@ \ref{\detokenize{common/technical_manual/framework:brailspipeline}}}, the framework consists of two steps: data fusion and data enhancement.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{pipeline}.png}
\caption{Framework}\label{\detokenize{common/technical_manual/framework:id12}}\label{\detokenize{common/technical_manual/framework:brailspipeline}}\end{figure}

\sphinxAtStartPar
Due to the complexity and the size of source data and the cost to collect it,
building information at the regional\sphinxhyphen{}scale is usually not able to be inferred
from a single resource but multiple resources, such as images, point clouds,
property tax records, crowd sourcing map, etc. And these data usually belong
to different owners and stored in different formats.
The framework combines these sources using data fusion (or data integration),
which is the process of integrating multiple building information data to produce
more consistent, accurate, and useful information than that provided by any
individual data source.
The expectation is that fused building information data is more informative
and synthetic than the original data.

\sphinxAtStartPar
The product of data fusion is an initial building inventory,
in which a significant amount of building information is missing because of data scarcity.
For example, crowd sourcing maps have issues with completeness, especially for rural regions.
They are more complete in densely urbanized areas and targeted areas of humanitarian
mapping intervention. Similarly, a significant amount of property tax assessment
records are missing from administrative databases. The incompleteness issue is
common for almost all data sources. In this framework, the missing values will
be filled by modellings performed based on the incomplete initial database.


\subsubsection{Extracting building information from images}
\label{\detokenize{common/technical_manual/framework:extracting-building-information-from-images}}
\sphinxAtStartPar
An initial building inventory, containing basic indexing information such as addresses or coordinates of individual buildings,
and other basic descriptions such as year built and structure type, has been created in the previous section.
Based on the indexing information, satellite or street view images of each building can be retrieved using Google Maps API.
Then, the deep learning technique ConvNet is utilized to extract building features
(that don’t exist in the initial database) from these images.
ConvNet is a class of deep neural networks inspired by biological processes in that the connectivity pattern
between neurons resembles the organization of the animal visual cortex \sphinxcite{index:hubel1968receptive} and
is most commonly applied to analyzing images.
Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field.
The receptive fields of different neurons partially overlap such that they cover the entire visual field.

\sphinxAtStartPar
ConvNet is a supervised learning algorithm, which means the images need to be labeled for training.
Therefore the most important part is to build a labeled dataset.
OSM is a platform hosting real world infrastructure information labeled by human.
For a typical building, the information might be found in OSM includes: height, number of stories, structure type,
exterior material, footprint shape, usage, etc. These information are the valuable source for describing the built environment.
However, only a limit number of buildings are labeled in OSM.
In this study, the investigators propose to harvest these labels and associate them with images to build a database for deep learning.
The ConvNets trained on these database are used to predict building properties
when given any images containing unseen and unlabeled buildings.
This way, as long as the satellite/street view images of a city can be obtained,
a database of building information can be created. The pipeline to extract a specific building property
from images is listed as following:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Identify a visually comprehensible building property (e.g., exterior construction material) that is intended to be extracted.

\item {} 
\sphinxAtStartPar
Retrieve satellite/street view images of individual buildings from Google Map API

\item {} 
\sphinxAtStartPar
Label the retrieved images using tags (e.g., exterior construction material type) found in OSM

\item {} 
\sphinxAtStartPar
Train a ConvNet on the labeled images to classify between types

\item {} 
\sphinxAtStartPar
Apply the trained ConvNet to unlabeled satellite / street view images of buildings in the city of interest

\end{enumerate}

\sphinxAtStartPar
Repeat the above five steps for other building properties,
such as number of stories, structural type, etc., as long as they can be visually identified from images.
The ConvNet\sphinxhyphen{}identified building information is then merged into the initial database resulting in a more detailed inventory.

\sphinxAtStartPar
It should be noted that, due to reasons like heavy occlusions in images or bad viewpoints,
predictions from ConvNet are not always with acceptable confidence. To tackle this issue, in {\hyperref[\detokenize{common/technical_manual/framework:enhance}]{\sphinxcrossref{\DUrole{std,std-ref}{Data enhancement}}}},
a machine learning based approach will be employed to enhance the database.


\subsubsection{Data fusion}
\label{\detokenize{common/technical_manual/framework:data-fusion}}
\sphinxAtStartPar
In order to combine building information obtained form different sources,
a data fusion process is designed, as shown in \hyperref[\detokenize{common/technical_manual/framework:brailsfusion}]{Fig.\@ \ref{\detokenize{common/technical_manual/framework:brailsfusion}}}.
There are two starting points of the data flow pipeline: one is the address list and the other one is the building footprints.

\sphinxAtStartPar
The address list is used as the index for querying the supporting data sources (e.g., OSM, tax records, other user provided data, etc.).
Once raw data is fetched from these sources, it will be filtered and cleaned to remove duplicated properties and then merged while missing values represented by place holders.

\sphinxAtStartPar
The building footprint is an important supporting data the framework relies on.
The method that is commonly used to get building footprints at a large scale is semantic segmentation on high\sphinxhyphen{}resolution
satellite maps \sphinxcite{index:li2019semantic}\sphinxcite{index:zhao2018building}\sphinxcite{index:bischke2019multi}.
Since it is out of the focus of this study, instead of performing segmentation in the proposed framework on the fly,
the framework retrieves footprints from a dataset released by \sphinxcite{index:msfootprint}.
This dataset contains footprints for almost all buildings in the United States extracted from high\sphinxhyphen{}resolution satellite maps.
For regions outside of the United States, footprints can be inferred from satellite images using semantic segmentation methods in aforementioned  references.
Each geotagged address has a unique coordinate, therefore can be merged with corresponding building footprints.

\sphinxAtStartPar
Using address as the index, the aforementioned filtered and cleaned basic building information retrieved from
multiple data sources can now be merged into the main stream of the data flow pipeline. For buildings with missing information,
satellite and street view images of each building are then retrieved and fed into pretrained ConvNets,
and predictions on the building features such as number of stories, roof types, etc., can be obtained.
Merging the predicted values into the data stream results in the initial building database.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{Fusion}.png}
\caption{Framework}\label{\detokenize{common/technical_manual/framework:id13}}\label{\detokenize{common/technical_manual/framework:brailsfusion}}\end{figure}


\subsubsection{Data enhancement}
\label{\detokenize{common/technical_manual/framework:data-enhancement}}\label{\detokenize{common/technical_manual/framework:enhance}}
\sphinxAtStartPar
Note that, after the fusion, the initial building database is still incomplete.
Some of the reasons are:
firstly no data source is perfect and there are usually a considerable amount of missing items in them;
secondly some missing values, for example the year of construction of an individual building,
are either visually incomprehensible to a ConvNet,
or for some visually comprehensible features, for example the number of stories,
if the building is occluded by other objects, usually a tree or a car, in the image,
which happens quite often, the feature can not be predicted accurately by ConvNets.
These reasons leave gaps in the initial building database.
This section describes a machine learning \sphinxhyphen{} based data enhancement method to
fill the gaps in the data.

\sphinxAtStartPar
Rather than merely a random assortment of objects in space,
landscapes, natural resources, the human built environment,
and other objects on Earth have orders, which can be described using a
spatial patterns \sphinxhyphen{} a perceptual structure, placement,
or arrangement of objects and the space in between those objects.
Patterns may be recognized because of their distance,
maybe in a line or by a clustering of points, and other arrangement types.

\sphinxAtStartPar
Such kind of spatial patterns, i.e., the arrangement of individual buildings
in space and the geographic relationships among them, exist in the distribution of buildings, too.
Buildings, when built, usually have a relationship between each other, i.e.,
one building is located at a specific location is usually because of another.
They can be clustered or dispersed based on their attributes,
such as building type, value, construction material, etc., which are usually the
manifestation of the demographic characteristics of neighborhoods,
such as household income or race.
For example, as the city shown in the map \hyperref[\detokenize{common/technical_manual/framework:brailsmapsf}]{Fig.\@ \ref{\detokenize{common/technical_manual/framework:brailsmapsf}}},
there are areas denser with buildings than others and clusters of
certain types of building are easy to be found in certain regions.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{mapSF}.png}
\caption{Satellite view of buildings in San Francisco}\label{\detokenize{common/technical_manual/framework:id14}}\label{\detokenize{common/technical_manual/framework:brailsmapsf}}\end{figure}

\sphinxAtStartPar
The capability of evaluating spatial patterns is a prerequisite to understanding the
complicated spatial processes underlying the distribution of a phenomenon.

\sphinxAtStartPar
In spatial statistics, the semivariogram is a function describing the degree of spatial
dependence of a spatial random field or stochastic process.
As such, statistics of spatial semivariogram provide a useful indicator of spatial patterns.
Semivariogram is essentially  a meassure of the degree of dissimilarity between
observations as a function of distance. It equals to half the variance of two
random variables separated by a vector distance \(\boldsymbol{h}\)
\sphinxcite{index:goovaerts1997geostatistics}\sphinxcite{index:vanmarcke2010random}\sphinxcite{index:wang2017spatial}\sphinxcite{index:wang2017hybrid}.
\begin{equation}\label{equation:common/technical_manual/framework:eq:gamma}
\begin{split}\gamma (\boldsymbol{h})= \frac{1}{2}Var[Z(\boldsymbol{\mu}) - Z(\boldsymbol{\mu}+\boldsymbol{h})]\end{split}
\end{equation}
\sphinxAtStartPar
where \(Z(\boldsymbol{\mu})\) is the observation at a spatial location \(\mu\);
\(Z(\boldsymbol{\mu}+\boldsymbol{h})\) is the observation at a spatial location \(\boldsymbol{\mu}+\boldsymbol{h}\).

\sphinxAtStartPar
It is expected that buildings far away from each other will are more different
than buildings that are close to each other. Because based on the first rule of
geography that things close together are more similar than things far apart,
semivariogram is generally low when two locations are close to each other
(i.e. observations at each point are likely to be similar to each other.
Typically, semivariogram increases as the distance between the locations
grows until at some point the locations are considered independent of each other
and semi\sphinxhyphen{}variance no longer increases.
In the case of buildings,
semivariograms will give measures of how much two buildings will vary in attributes
(such as height, number of stories, etc.) regarding to the distance between those samples.

\sphinxAtStartPar
Semivariogram function is employed to explore the spatial patterns of
different building properties within a selected region.
The results show that buildings were indeed built following certain spatial patterns.
As a demonstration, the spatial semivariograms of two building properties,
number of stories and year of construction, are plotted in \hyperref[\detokenize{common/technical_manual/framework:numofstories-semivariogram}]{Fig.\@ \ref{\detokenize{common/technical_manual/framework:numofstories-semivariogram}}}
and \hyperref[\detokenize{common/technical_manual/framework:yearofbuilt-semivariogram}]{Fig.\@ \ref{\detokenize{common/technical_manual/framework:yearofbuilt-semivariogram}}}.
The horizontal axis represents the distance between a pair of buildings, while the vertical axis represents the dis\sphinxhyphen{}similarity of these buildings.
The semivariogram figures show that with the increase of the distance between any two buildings, the dis\sphinxhyphen{}similarity between them,
regrading to number of stories and the year of construction for an example, increased and then fluctuated. Apparently the incremental
relationship between the distance and dis\sphinxhyphen{}similarity is neither linear nor following any obvious rule.
Another note that deserves to be taken here is the curves revealed in \hyperref[\detokenize{common/technical_manual/framework:numofstories-semivariogram}]{Fig.\@ \ref{\detokenize{common/technical_manual/framework:numofstories-semivariogram}}} and \hyperref[\detokenize{common/technical_manual/framework:yearofbuilt-semivariogram}]{Fig.\@ \ref{\detokenize{common/technical_manual/framework:yearofbuilt-semivariogram}}}
are city\sphinxhyphen{} or region\sphinxhyphen{}specific, i.e., the semivariogram curve may reflect the truth of the region being investigated, and may not be exactly
correct or directly applicable for describing another region.
In other words, the spatial dependence of building features are regional\sphinxhyphen{}specific and the semivariogram curves vary regionally.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{correlation_numofstories}.png}
\caption{Spatial patterns of building information expressed in semivariogram of the number of stories (The horizontal axis represents the distance between a pair of buildings, while the vertical axis represents the dis\sphinxhyphen{}similarity of these buildings.) These curves are calculated based on a building dataset covering four coastal cities in the Atlantic County, New Jersey}\label{\detokenize{common/technical_manual/framework:id15}}\label{\detokenize{common/technical_manual/framework:numofstories-semivariogram}}\end{figure}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{correlation_yearbuilt}.png}
\caption{Spatial patterns of building information expressed in semivariogram of the year of construction (The horizontal axis represents the distance between a pair of buildings, while the vertical axis represents the dis\sphinxhyphen{}similarity of these buildings.) These curves are calculated based on a building dataset covering four coastal cities in the Atlantic County, New Jersey}\label{\detokenize{common/technical_manual/framework:id16}}\label{\detokenize{common/technical_manual/framework:yearofbuilt-semivariogram}}\end{figure}

\sphinxAtStartPar
Since the semivariograms (\hyperref[\detokenize{common/technical_manual/framework:numofstories-semivariogram}]{Fig.\@ \ref{\detokenize{common/technical_manual/framework:numofstories-semivariogram}}} and \hyperref[\detokenize{common/technical_manual/framework:yearofbuilt-semivariogram}]{Fig.\@ \ref{\detokenize{common/technical_manual/framework:yearofbuilt-semivariogram}}} )
clearly show there is a spatial pattern of the distribution of a certain building property,
there must be a function for mapping neighbor information \(\boldsymbol{Z}_{p}\) into \(Z_{n}\).
This function can be constructed implicitly using a neural network.

\sphinxAtStartPar
Imagine a neighborhood consisting of three buildings, \hyperref[\detokenize{common/technical_manual/framework:neighborhood}]{Fig.\@ \ref{\detokenize{common/technical_manual/framework:neighborhood}}}.
Pretrained ConvNets can easily extract attributes of building at two ends,
such as number of stories, occupancy, structure type, etc.
However, for the building in the middle, which is heavily occluded by a tree in this case,
no information can be extracted from the image with a satisfying confidence.
However, it is possible to predict the features of the building in the middle based on the information of its neighbors,
because \hyperref[\detokenize{common/technical_manual/framework:numofstories-semivariogram}]{Fig.\@ \ref{\detokenize{common/technical_manual/framework:numofstories-semivariogram}}} and \hyperref[\detokenize{common/technical_manual/framework:yearofbuilt-semivariogram}]{Fig.\@ \ref{\detokenize{common/technical_manual/framework:yearofbuilt-semivariogram}}}  indicates that
the attributes of buildings within a community are correlated with each other.
The correlations can be learned by neural networks using \sphinxhref{https://github.com/NHERI-SimCenter/SURF}{SURF}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{neighbor}.png}
\caption{A neighborhood street view}\label{\detokenize{common/technical_manual/framework:id17}}\label{\detokenize{common/technical_manual/framework:neighborhood}}\end{figure}

\sphinxAtStartPar
As mentioned in the previous sections,
there is a significant portion of building information still missing from the
initial database or can not be extracted from images.
Using \sphinxhref{https://github.com/NHERI-SimCenter/SURF}{SURF},
the missing values can be predicted based on known values of neighboring buildings,
hence the gaps in the initial database are filled and the regional building information database is enhanced.
Details about how to do this can be found in the documentation of \sphinxhref{https://github.com/NHERI-SimCenter/SURF}{SURF}.

\sphinxAtStartPar



\subsection{Modules}
\label{\detokenize{common/technical_manual/modulesTheory:modules}}\label{\detokenize{common/technical_manual/modulesTheory:modulestheory}}\label{\detokenize{common/technical_manual/modulesTheory::doc}}

\subsubsection{Roof type classifier}
\label{\detokenize{common/technical_manual/roofTheory:roof-type-classifier}}\label{\detokenize{common/technical_manual/roofTheory:rooftheory}}\label{\detokenize{common/technical_manual/roofTheory::doc}}
\sphinxAtStartPar
Roof type is a crucial information for wind hazard analyses of buildings because it is
a key attribute needed for consideration of wind effects on structures.

\sphinxAtStartPar
There are three major roof types, as shown in \hyperref[\detokenize{common/technical_manual/roofTheory:roof-types}]{Table \ref{\detokenize{common/technical_manual/roofTheory:roof-types}}}, that are widely used in the world: flat, gabled, hipped.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Roof prototypes}\label{\detokenize{common/technical_manual/roofTheory:id2}}\label{\detokenize{common/technical_manual/roofTheory:roof-types}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{flat}.jpg}
\sphinxfigcaption{Flat}\label{\detokenize{common/technical_manual/roofTheory:id3}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{gable}.jpg}
\sphinxfigcaption{Gabled}\label{\detokenize{common/technical_manual/roofTheory:id4}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{hip}.jpg}
\sphinxfigcaption{Hipped}\label{\detokenize{common/technical_manual/roofTheory:id5}}\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Correspondingly, a typical satellite image of each roof type is shown in \hyperref[\detokenize{common/technical_manual/roofTheory:roof-images}]{Table \ref{\detokenize{common/technical_manual/roofTheory:roof-images}}}.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Example satellite images of different roof types}\label{\detokenize{common/technical_manual/roofTheory:id6}}\label{\detokenize{common/technical_manual/roofTheory:roof-images}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{94}.png}
\sphinxfigcaption{Flat}\label{\detokenize{common/technical_manual/roofTheory:id7}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{76}.png}
\sphinxfigcaption{Gabled}\label{\detokenize{common/technical_manual/roofTheory:id8}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{54}.png}
\sphinxfigcaption{Hipped}\label{\detokenize{common/technical_manual/roofTheory:id9}}\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Satellite images are a scalable source for inferring roof type information.
In the attempt to determine roof type for every building in a region,
a ConvNet classifier
is trained to take a satellite image of a building and predicts its roof type.
A training data set of 6,000 satellite images (2,000 for each roof type: flat, gabled, hipped) is collected.
Specifically,  ResNet \sphinxcite{index:he2016deep}, which is a widely\sphinxhyphen{}used ConvNet architecture for image feature recognition,
is employed.

\sphinxAtStartPar
The architecture of the model is shown in \hyperref[\detokenize{common/technical_manual/softstoryTheory:fig-resnet}]{Fig.\@ \ref{\detokenize{common/technical_manual/softstoryTheory:fig-resnet}}}.
In this module, we used a 50\sphinxhyphen{}layer ResNet.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=0.700\linewidth]{{ResNet}.png}
\caption{ResNet}\label{\detokenize{common/technical_manual/roofTheory:id10}}\label{\detokenize{common/technical_manual/roofTheory:fig-resnet}}\end{figure}


\subsubsection{Occupancy classifier}
\label{\detokenize{common/technical_manual/occupancyTheory:occupancy-classifier}}\label{\detokenize{common/technical_manual/occupancyTheory:occupancytheory}}\label{\detokenize{common/technical_manual/occupancyTheory::doc}}
\sphinxAtStartPar
Occupancy class is an important parameter for natural hazard risk analysis of a building.
The occupancy model in the current version of BRAILS is trained on a dataset consists of residential buildings
that are classified into three categories: single\sphinxhyphen{}family buildings (RES1),
multi\sphinxhyphen{}family buildings (RES3) and commercial buildings (COM). Data of other occupancy classes are being collected and will be added to the new versions of BRAILS.

\sphinxAtStartPar
Examples of different occupancy type is shown in \hyperref[\detokenize{common/technical_manual/occupancyTheory:occupancyexample}]{Table \ref{\detokenize{common/technical_manual/occupancyTheory:occupancyexample}}}.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Examples of different occupancy types}\label{\detokenize{common/technical_manual/occupancyTheory:id2}}\label{\detokenize{common/technical_manual/occupancyTheory:occupancyexample}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{RES1}.jpg}
\sphinxfigcaption{Single\sphinxhyphen{}Family Building}\label{\detokenize{common/technical_manual/occupancyTheory:id3}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{RES3}.jpg}
\sphinxfigcaption{Multi\sphinxhyphen{}Family Building}\label{\detokenize{common/technical_manual/occupancyTheory:id4}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{42915}.jpg}
\sphinxfigcaption{Commercial Building}\label{\detokenize{common/technical_manual/occupancyTheory:id5}}\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
The classifier is trained using a 50\sphinxhyphen{}layer ResNet \sphinxcite{index:he2016deep}, a widely used ConvNet architecture for images feature recognition.

\sphinxAtStartPar
Its architecture is shown in \hyperref[\detokenize{common/technical_manual/softstoryTheory:fig-resnet}]{Fig.\@ \ref{\detokenize{common/technical_manual/softstoryTheory:fig-resnet}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=0.700\linewidth]{{ResNet}.png}
\caption{ResNet}\label{\detokenize{common/technical_manual/occupancyTheory:id6}}\label{\detokenize{common/technical_manual/occupancyTheory:fig-resnet}}\end{figure}


\subsubsection{Soft\sphinxhyphen{}story}
\label{\detokenize{common/technical_manual/softstoryTheory:soft-story}}\label{\detokenize{common/technical_manual/softstoryTheory:softstorytheory}}\label{\detokenize{common/technical_manual/softstoryTheory::doc}}
\sphinxAtStartPar
Structural deficiencies identification is a crucial task in seismic risk evaluation of buildings.
In case of multi\sphinxhyphen{}story structures, abrupt vertical variations of story stiffness are known to significantly increase the likelihood of collapse during earthquakes.
These buildings are called soft story buildings.
Identifying these buildings is vital in earthquake loss estimation and mitigation.

\sphinxAtStartPar
One example of soft\sphinxhyphen{}story failure is shown in \hyperref[\detokenize{common/technical_manual/softstoryTheory:ssexample}]{Table \ref{\detokenize{common/technical_manual/softstoryTheory:ssexample}}}.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Soft\sphinxhyphen{}story failure}\label{\detokenize{common/technical_manual/softstoryTheory:id2}}\label{\detokenize{common/technical_manual/softstoryTheory:ssexample}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics[width=0.600\linewidth]{{ss}.png}
\sphinxfigcaption{Soft\sphinxhyphen{}story collapse}\label{\detokenize{common/technical_manual/softstoryTheory:id3}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics[width=0.600\linewidth]{{ss-frame}.png}
\sphinxfigcaption{Failure mechanism}\label{\detokenize{common/technical_manual/softstoryTheory:id4}}\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
The classifier is trained using a 50\sphinxhyphen{}layer ResNet \sphinxcite{index:he2016deep}, a widely used ConvNet architecture for images feature recognition.

\sphinxAtStartPar
Its architecture is shown in \hyperref[\detokenize{common/technical_manual/softstoryTheory:fig-resnet}]{Fig.\@ \ref{\detokenize{common/technical_manual/softstoryTheory:fig-resnet}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=0.700\linewidth]{{ResNet}.png}
\caption{ResNet}\label{\detokenize{common/technical_manual/softstoryTheory:id5}}\label{\detokenize{common/technical_manual/softstoryTheory:fig-resnet}}\end{figure}


\subsubsection{Number of Floors Detector}
\label{\detokenize{common/technical_manual/nfloorTheory:number-of-floors-detector}}\label{\detokenize{common/technical_manual/nfloorTheory:nfloortheory}}\label{\detokenize{common/technical_manual/nfloorTheory::doc}}
\sphinxAtStartPar
The number of Floors Detector is implemented based on object detection \sphinxhyphen{} each floor of a building is detected as a target object.

\sphinxAtStartPar
In general, all modern object detectors can be said to consist of three main components:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
A backbone network that extracts features from the given image at different scales,

\item {} 
\sphinxAtStartPar
A feature network that receives multiple levels of features from the backbone and returns a list of fused features that identify the dominant features of the image,

\item {} 
\sphinxAtStartPar
A class and box network that takes the fused features as input to predict the class and location of each object, respectively.

\end{enumerate}

\sphinxAtStartPar
EfficientDet models use EfficientNets pretrained on ImageNet for their backbone network. For the feature network, EfficienDet models use a novel bidirectional feature pyramid network (BiFPN), which takes level 3 through 7 features from the backbone network and repeatedly fuses these features in top\sphinxhyphen{}down and bottom\sphinxhyphen{}up directions. Both BiFPN layers and class/box layers are repeated multiple times with the number of repetations depending on the compund coefficient of the architecture. \hyperref[\detokenize{common/technical_manual/nfloorTheory:fig-modelarch}]{Fig.\@ \ref{\detokenize{common/technical_manual/nfloorTheory:fig-modelarch}}}  provides and overview of the described structure. For further details please see the seminal work by \sphinxhref{https://arxiv.org/abs/1911.09070}{Tan, Pang, and Le}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=0.700\linewidth]{{EfficientDetArch}.png}
\caption{A high\sphinxhyphen{}level representation of the EfficientDet architecture}\label{\detokenize{common/technical_manual/nfloorTheory:id1}}\label{\detokenize{common/technical_manual/nfloorTheory:fig-modelarch}}\end{figure}

\sphinxAtStartPar
Remarkable performance gains can be attained in image classification by jointly scaling up all dimensions of neural network width, depth, and input resolution, as noted in the study by \sphinxhref{https://arxiv.org/abs/1905.11946}{Tan and Le}. Inspired by this work, EfficienDet utilizes a new compound scaling method for object detection that jointly increases all dimensions of the backbone network, BiFPN, class/box network, and input image resolution, using a simple compound coefficient, φ. A total of 8 compounding levels are defined for EffcienDet, i.e., φ = 0 to 8, with EfficientDet\sphinxhyphen{}D0 being the simplest and EfficientDet\sphinxhyphen{}D8 being the most complex of the network architectures.

\sphinxAtStartPar
As shown in \hyperref[\detokenize{common/technical_manual/nfloorTheory:fig-efficientdetperf}]{Fig.\@ \ref{\detokenize{common/technical_manual/nfloorTheory:fig-efficientdetperf}}}, at the time this work was published, EfficientDet object detection algorithms attained the state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art performance on the COCO dataset. Also suggested in Figure 3 is the more complex the network architecture is, the higher the detection performance will be. From a practical standpoint, however, architecture selection will depend on the availability of computational resources. For example, to train a model on an architecture with a compound coefficient higher than 4, a GPU with a memory of more than 11 GB will almost always be required.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=0.700\linewidth]{{EfficientDetPerf}.png}
\caption{A comparison of the performance and accuracy levels of EfficienDet models over other popular object detection architectures on the COCO dataset}\label{\detokenize{common/technical_manual/nfloorTheory:id2}}\label{\detokenize{common/technical_manual/nfloorTheory:fig-efficientdetperf}}\end{figure}


\section{Validation}
\label{\detokenize{common/technical_manual/vnv:validation}}\label{\detokenize{common/technical_manual/vnv:lbl-vnv}}\label{\detokenize{common/technical_manual/vnv::doc}}
\sphinxAtStartPar
This section provides the validation test results for the following ML modules:


\subsection{Roof Classifier}
\label{\detokenize{common/technical_manual/roof:roof-classifier}}\label{\detokenize{common/technical_manual/roof:lbl-roofclassifier-vnv}}\label{\detokenize{common/technical_manual/roof::doc}}
\sphinxAtStartPar
The Roof Classifier’s methodology has been presented in {\hyperref[\detokenize{common/technical_manual/roofTheory:rooftheory}]{\sphinxcrossref{\DUrole{std,std-ref}{Roof type classifier}}}}, and examples showing how to use it can be found in {\hyperref[\detokenize{common/user_manual/modules/roofClassifier:lbl-roofclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{Roof Shape Classifier}}}}.
This section presents its validation against three datasets.


\subsubsection{Dataset 1: Compare with OpenStreetMap Labels}
\label{\detokenize{common/technical_manual/roof:dataset-1-compare-with-openstreetmap-labels}}
\sphinxAtStartPar
The trained classifier is first tested on a ground truth dataset that can be downloaded from \sphinxhref{http://doi.org/10.5281/zenodo.4520781}{here}.
We firstly obtained a set of randomly selected buildings in the United States with their roof type labelled on OpenStreetMap.
We then downloaded the satellite images from Google Maps for each building.
We removed images in which we didn’t clearly see there is a roof, examples are shown in \hyperref[\detokenize{common/technical_manual/roof:random-roof-empty}]{Table \ref{\detokenize{common/technical_manual/roof:random-roof-empty}}}.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Images removed from the test dataset}\label{\detokenize{common/technical_manual/roof:id3}}\label{\detokenize{common/technical_manual/roof:random-roof-empty}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx-76.78538744x38.978670699999995}.png}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx-76.84497884x38.71229606}.png}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx-76.93814628x39.06148106}.png}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx-76.86937465999999x39.10108044}.png}
\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
The resulting dataset contains roof images in three categories: 32 flat, 40 gabled, 52 hipped.
Examples of these satellite images can be found in {\hyperref[\detokenize{common/technical_manual/roofTheory:rooftheory}]{\sphinxcrossref{\DUrole{std,std-ref}{Roof type classifier}}}}.

\sphinxAtStartPar
Run the following python script to test on this dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{shutil}
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{from} \PYG{n+nn}{glob} \PYG{k+kn}{import} \PYG{n}{glob}
\PYG{k+kn}{import} \PYG{n+nn}{wget}
\PYG{k+kn}{import} \PYG{n+nn}{zipfile}

\PYG{c+c1}{\PYGZsh{} download the testing dataset}
\PYG{n}{wget}\PYG{o}{.}\PYG{n}{download}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://zenodo.org/record/4520781/files/satellite\PYGZhy{}images\PYGZhy{}val.zip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{with} \PYG{n}{zipfile}\PYG{o}{.}\PYG{n}{ZipFile}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{satellite\PYGZhy{}images\PYGZhy{}val.zip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{zip\PYGZus{}ref}\PYG{p}{:}
    \PYG{n}{zip\PYGZus{}ref}\PYG{o}{.}\PYG{n}{extractall}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} get images}
\PYG{n}{flatList} \PYG{o}{=} \PYG{n}{glob}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{satellite\PYGZhy{}images\PYGZhy{}val/flat/*.png}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{gabledList} \PYG{o}{=} \PYG{n}{glob}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{satellite\PYGZhy{}images\PYGZhy{}val/gabled/*.png}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{hippedList} \PYG{o}{=} \PYG{n}{glob}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{satellite\PYGZhy{}images\PYGZhy{}val/hipped/*.png}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} define the paths of images in a list}
\PYG{n}{imgs}\PYG{o}{=}\PYG{n}{flatList}\PYG{o}{+}\PYG{n}{gabledList}\PYG{o}{+}\PYG{n}{hippedList}

\PYG{c+c1}{\PYGZsh{} import the module}
\PYG{k+kn}{from} \PYG{n+nn}{brails}\PYG{n+nn}{.}\PYG{n+nn}{modules} \PYG{k+kn}{import} \PYG{n}{RoofClassifier}

\PYG{c+c1}{\PYGZsh{} initialize a roof classifier}
\PYG{n}{roofModel} \PYG{o}{=} \PYG{n}{RoofClassifier}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} use the model to predict}
\PYG{n}{predictions} \PYG{o}{=} \PYG{n}{roofModel}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{imgs}\PYG{p}{)}

\PYG{n}{prediction} \PYG{o}{=} \PYG{n}{predictions}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{prediction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{label} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{flat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{*}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{flatList}\PYG{p}{)} \PYG{o}{+} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gabled}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{*}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{gabledList}\PYG{p}{)} \PYG{o}{+} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hipped}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{*}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{hippedList}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} lot results}
\PYG{n}{class\PYGZus{}names} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{flat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gabled}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hipped}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{k+kn}{from} \PYG{n+nn}{brails}\PYG{n+nn}{.}\PYG{n+nn}{utils}\PYG{n+nn}{.}\PYG{n+nn}{plotUtils} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{confusion\PYGZus{}matrix}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{f1\PYGZus{}score}\PYG{p}{,}\PYG{n}{accuracy\PYGZus{}score}

\PYG{c+c1}{\PYGZsh{} print}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ Accuracy is   : }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{, Random guess is 0.33}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{prediction}\PYG{p}{,}\PYG{n}{label}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{cnf\PYGZus{}matrix} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{prediction}\PYG{p}{,}\PYG{n}{label}\PYG{p}{)}
\PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{,} \PYG{n}{classes}\PYG{o}{=}\PYG{n}{class\PYGZus{}names}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Confusion matrix}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{xlabel}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Labels}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predictions}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
The prediction accuracy on this dataset is 90.3\%. Precision is 90.3\%. Recall is 90.3\%. F1 is 90.3\%.

\sphinxAtStartPar
The confusion matrix for this validation is shown in \hyperref[\detokenize{common/technical_manual/roof:fig-confusion-roof}]{Fig.\@ \ref{\detokenize{common/technical_manual/roof:fig-confusion-roof}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=0.400\linewidth]{{confusion_roof}.png}
\caption{Confusion matrix \sphinxhyphen{} Roof type classification for OpenStreetMap}\label{\detokenize{common/technical_manual/roof:id4}}\label{\detokenize{common/technical_manual/roof:fig-confusion-roof}}\end{figure}


\subsubsection{Dataset 2: Compare with StEER Hurricane Laura Dataset}
\label{\detokenize{common/technical_manual/roof:dataset-2-compare-with-steer-hurricane-laura-dataset}}
\sphinxAtStartPar
The second validation dataset is from StEER.
This dataset contains satellite images of building, most are taken before Hurricane Laura.

\sphinxAtStartPar
From StEER, we obtained a list of addresses with their roof types labelled.
For each address, we downloaded an satellite image from Google Maps Static API.

\sphinxAtStartPar
Examples of these satellite images can be found in {\hyperref[\detokenize{common/technical_manual/roofTheory:rooftheory}]{\sphinxcrossref{\DUrole{std,std-ref}{Roof type classifier}}}}.

\sphinxAtStartPar
The labeling system of StEER is different from the BRAILS roof classification system.
The StEER labels include the following classes:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Gable/Hip Combo

\item {} 
\sphinxAtStartPar
Hip

\item {} 
\sphinxAtStartPar
Gable

\item {} 
\sphinxAtStartPar
Complex

\item {} 
\sphinxAtStartPar
Flat

\end{itemize}

\sphinxAtStartPar
The BRAILS roof types include the following classes:
\begin{itemize}
\item {} 
\sphinxAtStartPar
gabled

\item {} 
\sphinxAtStartPar
hipped

\item {} 
\sphinxAtStartPar
flat

\end{itemize}

\sphinxAtStartPar
To compare these two systems, we selected addresses labeled as ‘Flat’, ‘Gable’, ‘Hip’ from StEER.
As a result we got the following numbers of StEER labels:
\begin{itemize}
\item {} 
\sphinxAtStartPar
hipped,    33

\item {} 
\sphinxAtStartPar
gabled,    21

\item {} 
\sphinxAtStartPar
flat  ,     2

\end{itemize}

\sphinxAtStartPar
Download the labels, images, scripts for this validation from \sphinxhref{https://zenodo.org/record/4768487/files/Laura\_roof\_validation.zip}{here}.

\sphinxAtStartPar
The following shows the script to run this validation.
At the end, the script will plot a confusion matrix and print the accuracy.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} \PYGZsh{}\PYGZsh{}\PYGZsh{} Load labels and predictions from The Lake Charles Testbed}

\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StEER\PYGZus{}Laura.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(StEER)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{o}{|}
              \PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(StEER)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gable}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{o}{|}
              \PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(StEER)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Flat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}

\PYG{n}{roofDict} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gable}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gabled}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Flat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{flat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hipped}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}
\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(StEER)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{=}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(StEER)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{roofDict}\PYG{p}{[}\PYG{n}{x}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(Testbed)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{=}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(Testbed)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{roofDict}\PYG{p}{[}\PYG{n}{x}\PYG{p}{]}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} \PYGZsh{}\PYGZsh{}\PYGZsh{} Plot confusion matrix}

\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{n}{sys}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k+kn}{from} \PYG{n+nn}{plotUtils} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{confusion\PYGZus{}matrix}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{f1\PYGZus{}score}\PYG{p}{,}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{,}\PYG{n}{f1\PYGZus{}score}

\PYG{n}{class\PYGZus{}names} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(Testbed)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{predictions} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(Testbed)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{labels} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(StEER)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{cnf\PYGZus{}matrix} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{predictions}\PYG{p}{)}
\PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{,} \PYG{n}{classes}\PYG{o}{=}\PYG{n}{class\PYGZus{}names}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Confusion matrix}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{xlabel}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BRAILS}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StEER}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i}\PYG{p}{,}\PYG{n}{cname} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{class\PYGZus{}names}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{accuracy} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}.1f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{\PYGZpc{}}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{/}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{TP} \PYG{o}{=} \PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}
    \PYG{n}{FP} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{p}{]}
    \PYG{n}{FN} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{p}{]}
    \PYG{n}{F1} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}.1f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{\PYGZpc{}}\PYG{p}{(}\PYG{n}{TP}\PYG{o}{/}\PYG{p}{(}\PYG{n}{TP}\PYG{o}{+}\PYG{l+m+mf}{0.5}\PYG{o}{*}\PYG{p}{(}\PYG{n}{FP}\PYG{o}{+}\PYG{n}{FN}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{cname}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{: Accuracy = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{accuracy}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, F1 = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{F1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} \PYGZsh{}\PYGZsh{}\PYGZsh{} Copy images to directories \PYGZob{}label\PYGZcb{}\PYGZhy{}\PYGZob{}prediction\PYGZcb{} for inspection}

\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{shutil}

\PYG{n}{predDir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tmp/images/roof\PYGZus{}predictions}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{n}{predDir}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{makedirs}\PYG{p}{(}\PYG{n}{predDir}\PYG{p}{)}

\PYG{n}{falseNames} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{def} \PYG{n+nf}{copyfiles}\PYG{p}{(}\PYG{n}{bim}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{for} \PYG{n}{ind}\PYG{p}{,} \PYG{n}{row} \PYG{o+ow}{in} \PYG{n}{bim}\PYG{o}{.}\PYG{n}{iterrows}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{label} \PYG{o}{=} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(StEER)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
        \PYG{n}{pred} \PYG{o}{=} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(Testbed)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

        \PYG{n}{lon}\PYG{p}{,} \PYG{n}{lat} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}.6f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{\PYGZpc{}}\PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Longitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}.6f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{\PYGZpc{}}\PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Latitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

        \PYG{n}{oldfile} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tmp/images/TopView/TopViewx}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{lon}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{x}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{lat}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{.png}\PYG{l+s+s1}{\PYGZsq{}}
        \PYG{n}{newfile} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{predDir}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{/}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{label}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{pred}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{/TopViewx}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{lon}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{x}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{lat}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{.png}\PYG{l+s+s1}{\PYGZsq{}}

        \PYG{n}{thisFileDir} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{predDir}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{/}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{label}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{pred}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{/}\PYG{l+s+s1}{\PYGZsq{}}
        \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{n}{thisFileDir}\PYG{p}{)}\PYG{p}{:} \PYG{n}{os}\PYG{o}{.}\PYG{n}{makedirs}\PYG{p}{(}\PYG{n}{thisFileDir}\PYG{p}{)}

        \PYG{k}{try}\PYG{p}{:}
            \PYG{n}{shutil}\PYG{o}{.}\PYG{n}{copyfile}\PYG{p}{(}\PYG{n}{oldfile}\PYG{p}{,} \PYG{n}{newfile}\PYG{p}{)}
        \PYG{k}{except}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{oldfile}\PYG{p}{)}

\PYG{n}{copyfiles}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
In the files you downloaded, there are folders with names like gabled\sphinxhyphen{}hipped, which means those are images that are labelled as ‘gabled’ in StEER dataset,
but they are predicted as ‘hipped’. You can browse those images to investigate deeper.

\sphinxAtStartPar
The confusion matrix tested on this dataset is shown in \hyperref[\detokenize{common/technical_manual/roof:fig-confusion-roof-laura}]{Fig.\@ \ref{\detokenize{common/technical_manual/roof:fig-confusion-roof-laura}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=0.400\linewidth]{{fig_confusion_roof_laura}.png}
\caption{Confusion matrix \sphinxhyphen{} Roof type classification for Hurricane Laura}\label{\detokenize{common/technical_manual/roof:id5}}\label{\detokenize{common/technical_manual/roof:fig-confusion-roof-laura}}\end{figure}

\sphinxAtStartPar
The accuracy for three classes are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
flat: Accuracy = 1.0, F1 = 0.4

\item {} 
\sphinxAtStartPar
hipped: Accuracy = 0.7, F1 = 0.7

\item {} 
\sphinxAtStartPar
gabled: Accuracy = 0.7, F1 = 0.8

\end{itemize}


\subsubsection{Dataset 3: Compare with StEER Hurricane Irma Dataset with Post\sphinxhyphen{}disaster Images}
\label{\detokenize{common/technical_manual/roof:dataset-3-compare-with-steer-hurricane-irma-dataset-with-post-disaster-images}}


\sphinxAtStartPar
The third validation dataset is also from StEER.
This dataset contains satellite images of building that are taken after Hurricane Irma and labels of roof shapes.

\sphinxAtStartPar
From StEER, we obtained a list of addresses with their roof types labelled.
For each address, we downloaded an satellite image from Google Maps Static API.
It should be noted that these images were taken after the Hurricane Irma.

\sphinxAtStartPar
\DUrole{red}{These post\sphinxhyphen{}disaster images are very different from the training dataset, in which all images are taken before the disasters. Keep in mind, the BRAILS roof model is not designed to recognize the post\sphinxhyphen{}disaster images. The aim of this validation is to see how BRAILS performs on these post\sphinxhyphen{}disaster images, though the model is not trained for this purpose.}

\sphinxAtStartPar
Examples of these satellite images can be found in \hyperref[\detokenize{common/technical_manual/roof:irma-roof-examples}]{Table \ref{\detokenize{common/technical_manual/roof:irma-roof-examples}}}.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Example post\sphinxhyphen{}hurricane satellite images}\label{\detokenize{common/technical_manual/roof:id6}}\label{\detokenize{common/technical_manual/roof:irma-roof-examples}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx17346-Keystone-Road-Sugarloaf-Key-Monroe-Florida-}.png}
\sphinxfigcaption{A severely damaged building with its roof cover totally removed}\label{\detokenize{common/technical_manual/roof:id7}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx728-Grenada-Ln-Little-Torch-Key-Monroe-FL-}.png}
\sphinxfigcaption{A gabled roof with a temporary fix by partial covering with a blue tarp}\label{\detokenize{common/technical_manual/roof:id8}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx1114-Chokoloskee-Dr-Chokoloskee-Collier-FL-}.png}
\sphinxfigcaption{A building removed from the land, leaving an empty lot}\label{\detokenize{common/technical_manual/roof:id9}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx216-Goodland-Drive-East-Goodland-Collier-County-Florida-}.png}
\sphinxfigcaption{A building with a hipped roof that was not damaged}\label{\detokenize{common/technical_manual/roof:id10}}\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
The labeling system of StEER is different from the BRAILS roof classification system.
The StEER labels include the following classes:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Gable

\item {} 
\sphinxAtStartPar
Hip

\item {} 
\sphinxAtStartPar
Complex

\item {} 
\sphinxAtStartPar
Flat

\item {} 
\sphinxAtStartPar
Hip/Gable

\item {} 
\sphinxAtStartPar
Hip,Complex

\item {} 
\sphinxAtStartPar
Gable,Complex

\item {} 
\sphinxAtStartPar
Other

\item {} 
\sphinxAtStartPar
Gable,Flat

\item {} 
\sphinxAtStartPar
Hip/Gable,Complex

\item {} 
\sphinxAtStartPar
Hip,Flat

\item {} 
\sphinxAtStartPar
Gambrel

\item {} 
\sphinxAtStartPar
Hip,Gable

\item {} 
\sphinxAtStartPar
Hip,Other

\item {} 
\sphinxAtStartPar
Hip/Gable,Flat

\end{itemize}

\sphinxAtStartPar
The BRAILS roof types include the following classes:
\begin{itemize}
\item {} 
\sphinxAtStartPar
gabled

\item {} 
\sphinxAtStartPar
hipped

\item {} 
\sphinxAtStartPar
flat

\end{itemize}

\sphinxAtStartPar
To compare these two systems, we selected addresses labeled as ‘Flat’, ‘Gable’, ‘Hip’ from StEER.
As a result we got the following numbers of StEER labels:
\begin{itemize}
\item {} 
\sphinxAtStartPar
gabled,    459

\item {} 
\sphinxAtStartPar
hipped,    180

\item {} 
\sphinxAtStartPar
flat,       72

\end{itemize}

\sphinxAtStartPar
Download the labels, images, scripts for this validation from \sphinxhref{https://zenodo.org/record/4767858/files/Irma\_roof\_validation.zip}{here}.

\sphinxAtStartPar
The following shows the script to run this validation.
It will use BRAILS to download images from Google Map Static API and perform predictions on these images.
At the end, the script will plot a confusion matrix and print the accuracy.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Irma\PYGZus{}validation.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} \PYGZsh{}\PYGZsh{}\PYGZsh{} Use BRAILS to download satellite images}

\PYG{c+c1}{\PYGZsh{} You don\PYGZsq{}t have to download again, it\PYGZsq{}s already included in this example}
\PYG{c+c1}{\PYGZsh{} But feel free to uncomment and download again. You need a Google API Key, and set reDownloadImgs=True}
\PYG{l+s+sd}{\PYGZsq{}\PYGZsq{}\PYGZsq{}}
\PYG{l+s+sd}{from brails.workflow.Images import getGoogleImagesByAddrOrCoord}

\PYG{l+s+sd}{addrs =  list(data[\PYGZsq{}Addr\PYGZsq{}])}
\PYG{l+s+sd}{getGoogleImagesByAddrOrCoord(Addrs=addrs, GoogleMapAPIKey=\PYGZsq{}Your\PYGZhy{}Key\PYGZsq{},}
\PYG{l+s+sd}{                             imageTypes=[\PYGZsq{}TopView\PYGZsq{}],imgDir=\PYGZsq{}tmp/images\PYGZsq{},ncpu=2,}
\PYG{l+s+sd}{                             fov=60,pitch=0,reDownloadImgs=False)}
\PYG{l+s+sd}{\PYGZsq{}\PYGZsq{}\PYGZsq{}}
\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TopViewImg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tmp/images/TopView/TopViewx}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{+}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Addr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{+}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.png}\PYG{l+s+s1}{\PYGZsq{}}


\PYG{c+c1}{\PYGZsh{} \PYGZsh{}\PYGZsh{}\PYGZsh{} Predict}

\PYG{k+kn}{from} \PYG{n+nn}{brails}\PYG{n+nn}{.}\PYG{n+nn}{modules} \PYG{k+kn}{import} \PYG{n}{RoofClassifier}
\PYG{n}{roofModel} \PYG{o}{=} \PYG{n}{RoofClassifier}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{roofPreds} \PYG{o}{=} \PYG{n}{roofModel}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TopViewImg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(BRAILS)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{=}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{roofPreds}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{prediction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{prob\PYGZus{}RoofShape(BRAILS)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{=}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{roofPreds}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{probability}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{roofDict} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gable}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gabled}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Flat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{flat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hipped}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}
\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(StEER)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{=}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{roof\PYGZus{}shape}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{roofDict}\PYG{p}{[}\PYG{n}{x}\PYG{p}{]}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} \PYGZsh{}\PYGZsh{}\PYGZsh{} Plot confusion matrix}

\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{get\PYGZus{}ipython}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{run\PYGZus{}line\PYGZus{}magic}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{matplotlib}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{inline}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{sys}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k+kn}{from} \PYG{n+nn}{plotUtils} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{confusion\PYGZus{}matrix}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{f1\PYGZus{}score}\PYG{p}{,}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{,}\PYG{n}{f1\PYGZus{}score}

\PYG{n}{class\PYGZus{}names} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(BRAILS)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{predictions} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(BRAILS)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{labels} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(StEER)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{cnf\PYGZus{}matrix} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{predictions}\PYG{p}{,}\PYG{n}{labels}\PYG{o}{=}\PYG{n}{class\PYGZus{}names}\PYG{p}{)}
\PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{,} \PYG{n}{classes}\PYG{o}{=}\PYG{n}{class\PYGZus{}names}\PYG{p}{,} \PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{xlabel}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BRAILS}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StEER}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}


\PYG{k}{for} \PYG{n}{i}\PYG{p}{,}\PYG{n}{cname} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{class\PYGZus{}names}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{accuracy} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}.1f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{\PYGZpc{}}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{/}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{TP} \PYG{o}{=} \PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}
    \PYG{n}{FP} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{p}{]}
    \PYG{n}{FN} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{p}{]}
    \PYG{n}{F1} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}.1f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{\PYGZpc{}}\PYG{p}{(}\PYG{n}{TP}\PYG{o}{/}\PYG{p}{(}\PYG{n}{TP}\PYG{o}{+}\PYG{l+m+mf}{0.5}\PYG{o}{*}\PYG{p}{(}\PYG{n}{FP}\PYG{o}{+}\PYG{n}{FN}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{cname}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{: Accuracy = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{accuracy}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, F1 = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{F1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}



\PYG{c+c1}{\PYGZsh{} \PYGZsh{}\PYGZsh{}\PYGZsh{} Copy images to directories \PYGZob{}label\PYGZcb{}\PYGZhy{}\PYGZob{}prediction\PYGZcb{} for inspection}

\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{shutil}

\PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tmp/images/roof\PYGZus{}predictions/}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{makedirs}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tmp/images/roof\PYGZus{}predictions/}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ind}\PYG{p}{,} \PYG{n}{row} \PYG{o+ow}{in} \PYG{n}{data}\PYG{o}{.}\PYG{n}{iterrows}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{addrstr} \PYG{o}{=} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Addr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{picname} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tmp/images/TopView/TopViewx}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{addrstr}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{.png}\PYG{l+s+s1}{\PYGZsq{}}

    \PYG{n}{label} \PYG{o}{=} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(StEER)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{n}{pred} \PYG{o}{=} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RoofShape(BRAILS)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{k}{if} \PYG{k+kc}{True}\PYG{p}{:}
        \PYG{n}{thisFileDir} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tmp/images/roof\PYGZus{}predictions/}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{label}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{pred}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{/}\PYG{l+s+s1}{\PYGZsq{}}
        \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{n}{thisFileDir}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{os}\PYG{o}{.}\PYG{n}{makedirs}\PYG{p}{(}\PYG{n}{thisFileDir}\PYG{p}{)}
        \PYG{n}{shutil}\PYG{o}{.}\PYG{n}{copyfile}\PYG{p}{(}\PYG{n}{picname}\PYG{p}{,} \PYG{n}{thisFileDir}\PYG{o}{+}\PYG{n}{picname}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
In the files you downloaded, there are folders with names like gabled\sphinxhyphen{}hipped, which means those are images that are labelled as ‘gabled’ in StEER dataset,
but they are predicted as ‘hipped’. You can browse those images to investigate deeper.

\sphinxAtStartPar
The confusion matrix tested on this dataset is shown in \hyperref[\detokenize{common/technical_manual/roof:fig-confusion-roof-irma}]{Fig.\@ \ref{\detokenize{common/technical_manual/roof:fig-confusion-roof-irma}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=0.400\linewidth]{{fig_confusion_roof_irma}.png}
\caption{Confusion matrix \sphinxhyphen{} Roof type classification for Hurricane Irma}\label{\detokenize{common/technical_manual/roof:id11}}\label{\detokenize{common/technical_manual/roof:fig-confusion-roof-irma}}\end{figure}

\sphinxAtStartPar
The accuracy for three classes are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
gabled: Accuracy = 0.2, F1 = 0.3

\item {} 
\sphinxAtStartPar
hipped: Accuracy = 0.9, F1 = 0.6

\item {} 
\sphinxAtStartPar
flat:   Accuracy = 0.8, F1 = 0.3

\end{itemize}

\sphinxAtStartPar
Examples of false predictions are shown in \hyperref[\detokenize{common/technical_manual/roof:irma-roof-examples-false}]{Table \ref{\detokenize{common/technical_manual/roof:irma-roof-examples-false}}}.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Examples of false predictions}\label{\detokenize{common/technical_manual/roof:id12}}\label{\detokenize{common/technical_manual/roof:irma-roof-examples-false}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx35-Blue-Water-Drive-Key-West-Monroe-FL-}.png}
\sphinxfigcaption{Label: Gabled, BRAILS Prediction: Flat}\label{\detokenize{common/technical_manual/roof:id13}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx43-Blue-Water-Dr-Key-West-Monroe-FL-}.png}
\sphinxfigcaption{Label: Gabled, BRAILS Prediction: Flat}\label{\detokenize{common/technical_manual/roof:id14}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx13-Boulder-Dr-Key-West-Monroe-FL-}.png}
\sphinxfigcaption{Label: Gabled, BRAILS Prediction: Hipped}\label{\detokenize{common/technical_manual/roof:id15}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx208-North-Storter-Avenue-Everglades-City-Collier-County-Florida-}.png}
\sphinxfigcaption{Label: Gabled, BRAILS Prediction: Hipped}\label{\detokenize{common/technical_manual/roof:id16}}\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
It shows the accuracy for the gable is not as high as the other classes.
A further look into the images we found the following facts:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Most roofs that are labelled as ‘hipped’ are not damaged during the hurricane, examples are in \hyperref[\detokenize{common/technical_manual/roof:irma-roof-examples-hipped}]{Table \ref{\detokenize{common/technical_manual/roof:irma-roof-examples-hipped}}}.

\item {} 
\sphinxAtStartPar
Roofs labelled as ‘flat’ and ‘gabled’ are the classes got most damages, examples are in \hyperref[\detokenize{common/technical_manual/roof:irma-roof-examples-flat}]{Table \ref{\detokenize{common/technical_manual/roof:irma-roof-examples-flat}}} and \hyperref[\detokenize{common/technical_manual/roof:irma-roof-examples-gable}]{Table \ref{\detokenize{common/technical_manual/roof:irma-roof-examples-gable}}}.

\item {} 
\sphinxAtStartPar
When roofs are damaged, their satellite images are different from pre\sphinxhyphen{}disaster images, examples are shown in \hyperref[\detokenize{common/technical_manual/roof:irma-roof-examples-sever}]{Table \ref{\detokenize{common/technical_manual/roof:irma-roof-examples-sever}}}, This could cause difficulties to the model to recognize features.

\item {} 
\sphinxAtStartPar
This validation doesn’t remove those images with an empty lot, which negatively influences the accuracy.

\item {} 
\sphinxAtStartPar
Bias in dataset is very common. This validation doesn’t consider the possible bias in the StEER labels (examples can be found in \hyperref[\detokenize{common/technical_manual/roof:irma-roof-examples-bias}]{Table \ref{\detokenize{common/technical_manual/roof:irma-roof-examples-bias}}}), which also negatively influences the accuracy.

\end{enumerate}


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Examples of post\sphinxhyphen{}hurricane satellite images: hipped}\label{\detokenize{common/technical_manual/roof:id17}}\label{\detokenize{common/technical_manual/roof:irma-roof-examples-hipped}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx31-Jimmy-Mark-Pl-St.-Augustine-St.-Johns-FL-}.png}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx50-Jimmy-Mark-St.-Augustine-St.-Johns-FL-}.png}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx202-Datura-Street-East-Naples-Collier-County-Florida-}.png}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx295-Porter-St-Naples-Collier-FL-}.png}
\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Examples of post\sphinxhyphen{}hurricane satellite images: flat}\label{\detokenize{common/technical_manual/roof:id18}}\label{\detokenize{common/technical_manual/roof:irma-roof-examples-flat}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx518-Bay-Shore-Dr-Ramrod-Key-Monroe-FL-}.png}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx2000-Cocoa-Plum-Drive-Marathon-Monroe-County-Florida-}.png}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx30918-Edward-Road-Big-Pine-Key-Monroe-County-Florida-}.png}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx31108-Avenue-J-Big-Pine-Key-Monroe-County-Florida-}.png}
\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Examples of post\sphinxhyphen{}hurricane satellite images: gabled}\label{\detokenize{common/technical_manual/roof:id19}}\label{\detokenize{common/technical_manual/roof:irma-roof-examples-gable}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx3979-Gordon-Road-Big-Pine-Key-Monroe-County-Florida-}.png}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx2056-Coral-Way-Big-Pine-Key-Monroe-County-Florida-}.png}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx1231-6th-Ave-Marco-Island-Collier-FL-}.png}
\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics{{TopViewx877-Grenada-Lane-Little-Torch-Key-Monroe-FL-}.png}
\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Examples of post\sphinxhyphen{}hurricane satellite images: severely damaged buildings / empty lot}\label{\detokenize{common/technical_manual/roof:id20}}\label{\detokenize{common/technical_manual/roof:irma-roof-examples-sever}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx518-Bay-Shore-Dr-Ramrod-Key-Monroe-FL-1}.png}
\sphinxfigcaption{A severely damaged roof}\label{\detokenize{common/technical_manual/roof:id21}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx31353-Avenue-H-Big-Pine-Key-Monroe-County-Florida-}.png}
\sphinxfigcaption{A severely damaged roof}\label{\detokenize{common/technical_manual/roof:id22}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx31373-Avenue-C-Big-Pine-Key-Monroe-FL-}.png}
\sphinxfigcaption{A severely damaged roof}\label{\detokenize{common/technical_manual/roof:id23}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx57642-Overseas-Hwy-Grassy-Key-Monroe-FL-}.png}
\sphinxfigcaption{A severely damaged roof}\label{\detokenize{common/technical_manual/roof:id24}}\end{sphinxfigure-in-table}\relax
\\
\hline\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx193-Sunset-Drive-Islamorada-Monroe-County-Florida-}.png}
\sphinxfigcaption{An empty lot}\label{\detokenize{common/technical_manual/roof:id25}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx7905-NW-36th-St-Doral-Miami-Dade-FL-}.png}
\sphinxfigcaption{An empty lot}\label{\detokenize{common/technical_manual/roof:id26}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx31395-Avenue-G-Big-Pine-Key-Monroe-FL-}.png}
\sphinxfigcaption{An empty lot}\label{\detokenize{common/technical_manual/roof:id27}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx31465-Avenue-F-Big-Pine-Key-Monroe-County-Florida-}.png}
\sphinxfigcaption{An empty lot}\label{\detokenize{common/technical_manual/roof:id28}}\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Example of post\sphinxhyphen{}hurricane satellite images: Bias in the labels}\label{\detokenize{common/technical_manual/roof:id29}}\label{\detokenize{common/technical_manual/roof:irma-roof-examples-bias}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx37-Cherokee-Trail-Naples-Collier-County-Florida-}.png}
\sphinxfigcaption{Label: Flat, BRAILS Prediction: Gabled}\label{\detokenize{common/technical_manual/roof:id30}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx512-Coconut-Ave-Goodland-Collier-FL-}.png}
\sphinxfigcaption{Label: Flat, BRAILS Prediction: Gabled}\label{\detokenize{common/technical_manual/roof:id31}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx219-Northwest-109th-Avenue-Miami-Miami-Dade-County-Florida-}.png}
\sphinxfigcaption{Label: Gabled, BRAILS Prediction: Flat}\label{\detokenize{common/technical_manual/roof:id32}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx738-Grenada-Ln-Little-Torch-Key-Monroe-FL-}.png}
\sphinxfigcaption{Label: Gabled, BRAILS Prediction: Flat}\label{\detokenize{common/technical_manual/roof:id33}}\end{sphinxfigure-in-table}\relax
\\
\hline\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx40-Jimmy-Mark-Pl-St.-Augustine-St.-Johns-FL-}.png}
\sphinxfigcaption{Label: Gabled, BRAILS Prediction: Hipped}\label{\detokenize{common/technical_manual/roof:id34}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx237-Goodland-Dr-E-Goodland-Collier-FL-}.png}
\sphinxfigcaption{Label: Gabled, BRAILS Prediction: Hipped}\label{\detokenize{common/technical_manual/roof:id35}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx75-Seaview-Avenue-Conch-Key-Monroe-Florida-}.png}
\sphinxfigcaption{Label: Hipped, BRAILS Prediction: Flat}\label{\detokenize{common/technical_manual/roof:id36}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{TopViewx302-North-Storter-Avenue-Everglades-City-Collier-County-Florida-}.png}
\sphinxfigcaption{Label: Hipped, BRAILS Prediction: Gabled}\label{\detokenize{common/technical_manual/roof:id37}}\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{Occupancy Classifier}
\label{\detokenize{common/technical_manual/occupancy:occupancy-classifier}}\label{\detokenize{common/technical_manual/occupancy:lbl-occupancyclassifier-vnv}}\label{\detokenize{common/technical_manual/occupancy::doc}}
\sphinxAtStartPar
The Occupancy Classifier’s methodology has been presented in {\hyperref[\detokenize{common/technical_manual/occupancyTheory:occupancytheory}]{\sphinxcrossref{\DUrole{std,std-ref}{Occupancy classifier}}}}, and examples showing how to use it can be found in {\hyperref[\detokenize{common/user_manual/modules/occupancyClassifier:lbl-occupancyclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{Occupancy Classifier}}}}.
This section presents its validation against two datasets.


\subsubsection{Dataset 1: Compare with OpenStreetMap Labels}
\label{\detokenize{common/technical_manual/occupancy:dataset-1-compare-with-openstreetmap-labels}}
\sphinxAtStartPar
The trained classifier is tested on a ground truth dataset that can be downloaded from \sphinxhref{https://zenodo.org/record/4553803/files/occupancy\_validation\_images.zip}{here}.
We firstly obtained a set of randomly selected buildings in the United States with occupancy tags found on OpenStreetMap.
We then downloaded the street view images from Google Street View for each building.
We removed images in which we didn’t clearly see there is a building.
The dataset contains 98 single family buildings (RES1), 97 multi\sphinxhyphen{}family buildings (RES3) and 98 commercial buildings (COM).
Examples of these street view images can be found in {\hyperref[\detokenize{common/user_manual/modules/occupancyClassifier:lbl-occupancyclassifier}]{\sphinxcrossref{\DUrole{std,std-ref}{Occupancy Classifier}}}}.

\sphinxAtStartPar
Run the following python script to test on this dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} download the testing dataset}

\PYG{k+kn}{import} \PYG{n+nn}{wget}
\PYG{k+kn}{import} \PYG{n+nn}{zipfile}
\PYG{n}{wget}\PYG{o}{.}\PYG{n}{download}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://zenodo.org/record/4553803/files/occupancy\PYGZus{}validation\PYGZus{}images.zip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{with} \PYG{n}{zipfile}\PYG{o}{.}\PYG{n}{ZipFile}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{occupancy\PYGZus{}validation\PYGZus{}images.zip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{zip\PYGZus{}ref}\PYG{p}{:}
    \PYG{n}{zip\PYGZus{}ref}\PYG{o}{.}\PYG{n}{extractall}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} prepare the image lists}

\PYG{k+kn}{import} \PYG{n+nn}{shutil}
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{from} \PYG{n+nn}{glob} \PYG{k+kn}{import} \PYG{n}{glob}

\PYG{n}{class\PYGZus{}names} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RES3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{COM}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RES1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{labels} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{n}{images} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{clas} \PYG{o+ow}{in} \PYG{n}{class\PYGZus{}names}\PYG{p}{:}
    \PYG{n}{imgs} \PYG{o}{=} \PYG{n}{glob}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{occupancy\PYGZus{}validation\PYGZus{}images/}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{clas}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{/*.jpg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{img} \PYG{o+ow}{in} \PYG{n}{imgs}\PYG{p}{:}
        \PYG{n}{labels}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{clas}\PYG{p}{)}
        \PYG{n}{images}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{img}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} import the module}
\PYG{k+kn}{from} \PYG{n+nn}{brails}\PYG{n+nn}{.}\PYG{n+nn}{modules} \PYG{k+kn}{import} \PYG{n}{OccupancyClassifier}

\PYG{c+c1}{\PYGZsh{} initialize the classifier}
\PYG{n}{occupancyModel} \PYG{o}{=} \PYG{n}{OccupancyClassifier}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} use the model to predict}
\PYG{n}{pred} \PYG{o}{=} \PYG{n}{occupancyModel}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{images}\PYG{p}{)}
\PYG{n}{predictions} \PYG{o}{=} \PYG{n}{pred}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{prediction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot results}
\PYG{k+kn}{from} \PYG{n+nn}{brails}\PYG{n+nn}{.}\PYG{n+nn}{utils}\PYG{n+nn}{.}\PYG{n+nn}{plotUtils} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{confusion\PYGZus{}matrix}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{f1\PYGZus{}score}\PYG{p}{,}\PYG{n}{accuracy\PYGZus{}score}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ Accuracy is   : }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{, Random guess is 0.33}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{predictions}\PYG{p}{,}\PYG{n}{labels}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{cnf\PYGZus{}matrix} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{predictions}\PYG{p}{,}\PYG{n}{labels}\PYG{p}{)}
\PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{,} \PYG{n}{classes}\PYG{o}{=}\PYG{n}{class\PYGZus{}names}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Confusion matrix}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}\PYG{n}{xlabel}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Labels}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predictions}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
The confusion matrix tested on this dataset is shown in \hyperref[\detokenize{common/technical_manual/occupancy:fig-confusion-occupancyv2}]{Fig.\@ \ref{\detokenize{common/technical_manual/occupancy:fig-confusion-occupancyv2}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=0.400\linewidth]{{confusion_occupancy_v2}.png}
\caption{Confusion matrix \sphinxhyphen{} Occupancy Class classifier}\label{\detokenize{common/technical_manual/occupancy:id2}}\label{\detokenize{common/technical_manual/occupancy:fig-confusion-occupancyv2}}\end{figure}

\sphinxAtStartPar
The accuracy for the two classes are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
RES3: Accuracy = 0.97, F1 = 0.97

\item {} 
\sphinxAtStartPar
COM: Accuracy = 0.97, F1 = 0.98

\item {} 
\sphinxAtStartPar
RES1: Accuracy = 0.99, F1 = 0.97

\end{itemize}


\subsubsection{Dataset 2: Compare with NJDEP Dataset}
\label{\detokenize{common/technical_manual/occupancy:dataset-2-compare-with-njdep-dataset}}
\sphinxAtStartPar
The second validation dataset is from New Jersey Department of Environmental Protection (NJDEP).

\sphinxAtStartPar
NJDEP developed a building inventory for flood hazard and risk analysis as part of its flood control and resilience mission.
In this dataset, we can find building footprints with their occupancy types labelled.
We randomly selected a subset of those records, for each we downloaded a street view image from Google Maps Static API.

\sphinxAtStartPar
Examples of these satellite images can be found in {\hyperref[\detokenize{common/technical_manual/occupancyTheory:occupancytheory}]{\sphinxcrossref{\DUrole{std,std-ref}{Occupancy classifier}}}}.

\sphinxAtStartPar
The NJDEP occupancy data includes the following labels:
\begin{itemize}
\item {} 
\sphinxAtStartPar
RES1     26574

\item {} 
\sphinxAtStartPar
RES3A     1714

\item {} 
\sphinxAtStartPar
COM1      1110

\item {} 
\sphinxAtStartPar
RES3B     1016

\item {} 
\sphinxAtStartPar
RES3C      779

\item {} 
\sphinxAtStartPar
RES3D      566

\item {} 
\sphinxAtStartPar
COM8       187

\item {} 
\sphinxAtStartPar
AGR1       113

\item {} 
\sphinxAtStartPar
RES4       111

\item {} 
\sphinxAtStartPar
COM4       100

\item {} 
\sphinxAtStartPar
GOV1        90

\item {} 
\sphinxAtStartPar
IND2        83

\item {} 
\sphinxAtStartPar
COM3        74

\item {} 
\sphinxAtStartPar
REL1        67

\item {} 
\sphinxAtStartPar
RES3E       52

\item {} 
\sphinxAtStartPar
EDU1        48

\item {} 
\sphinxAtStartPar
IND3        37

\item {} 
\sphinxAtStartPar
GOV2        24

\item {} 
\sphinxAtStartPar
COM7        16

\item {} 
\sphinxAtStartPar
RES3F       15

\item {} 
\sphinxAtStartPar
IND1        13

\item {} 
\sphinxAtStartPar
EDU2        11

\item {} 
\sphinxAtStartPar
IND4        11

\item {} 
\sphinxAtStartPar
IND5         6

\item {} 
\sphinxAtStartPar
COM2         3

\item {} 
\sphinxAtStartPar
COM10        3

\item {} 
\sphinxAtStartPar
COM6         2

\item {} 
\sphinxAtStartPar
IND6         2

\item {} 
\sphinxAtStartPar
COM5         1

\end{itemize}

\sphinxAtStartPar
The BRAILS occupancy system include the following classes:
\begin{itemize}
\item {} 
\sphinxAtStartPar
RES1

\item {} 
\sphinxAtStartPar
RES3

\item {} 
\sphinxAtStartPar
COM

\end{itemize}

\sphinxAtStartPar
To compare these two systems, we renamed some NJDEP labels:
\begin{itemize}
\item {} 
\sphinxAtStartPar
RES1  \sphinxhyphen{}\textgreater{} RES1

\item {} 
\sphinxAtStartPar
RES3A \sphinxhyphen{}\textgreater{} RES3

\item {} 
\sphinxAtStartPar
RES3B \sphinxhyphen{}\textgreater{} RES3

\item {} 
\sphinxAtStartPar
RES3C \sphinxhyphen{}\textgreater{} RES3

\item {} 
\sphinxAtStartPar
RES3D \sphinxhyphen{}\textgreater{} RES3

\item {} 
\sphinxAtStartPar
RES3F \sphinxhyphen{}\textgreater{} RES3

\item {} 
\sphinxAtStartPar
RES3E \sphinxhyphen{}\textgreater{} RES3

\item {} 
\sphinxAtStartPar
COM1  \sphinxhyphen{}\textgreater{} COM

\item {} 
\sphinxAtStartPar
COM2  \sphinxhyphen{}\textgreater{} COM

\item {} 
\sphinxAtStartPar
COM3  \sphinxhyphen{}\textgreater{} COM

\item {} 
\sphinxAtStartPar
COM4  \sphinxhyphen{}\textgreater{} COM

\item {} 
\sphinxAtStartPar
COM5  \sphinxhyphen{}\textgreater{} COM

\item {} 
\sphinxAtStartPar
COM6  \sphinxhyphen{}\textgreater{} COM

\item {} 
\sphinxAtStartPar
COM7  \sphinxhyphen{}\textgreater{} COM

\item {} 
\sphinxAtStartPar
COM8  \sphinxhyphen{}\textgreater{} COM

\item {} 
\sphinxAtStartPar
COM10 \sphinxhyphen{}\textgreater{} COM

\end{itemize}

\sphinxAtStartPar
From the relabelled records, we selected the following for validation:
\begin{itemize}
\item {} 
\sphinxAtStartPar
RES1,    1,000 randomly selected from RES1

\item {} 
\sphinxAtStartPar
RES3,    1,000 randomly selected from RES3

\item {} 
\sphinxAtStartPar
COM,    1,000 randomly selected from COM

\end{itemize}

\sphinxAtStartPar
You can download the labels, images, scripts for this validation from \sphinxhref{https://zenodo.org/record/4774367/files/AtlanticCountyNJDEP\_Occupancy\_Validation.zip}{here}.

\sphinxAtStartPar
The following shows the script to run this validation.
At the end, the script will plot a confusion matrix and print the accuracy.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{AtlanticCountyBuildingInventory.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{getCls}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RES1}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{in} \PYG{n}{x}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RES1}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{elif} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RES3}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{in} \PYG{n}{x}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RES3}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{elif} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{COM}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o+ow}{in} \PYG{n}{x}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{COM}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{else}\PYG{p}{:} \PYG{k}{return} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{remove}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{occupancy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{=}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OccupancyClass}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{getCls}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}data=data[data[\PYGZsq{}occupancy\PYGZsq{}]!=\PYGZsq{}remove\PYGZsq{}]}
\PYG{n}{RES1} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{occupancy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RES1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{n}{random\PYGZus{}state} \PYG{o}{=} \PYG{l+m+mi}{1993}\PYG{p}{)}
\PYG{n}{RES3} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{occupancy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RES3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{n}{random\PYGZus{}state} \PYG{o}{=} \PYG{l+m+mi}{1993}\PYG{p}{)}
\PYG{n}{COM} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{occupancy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{COM}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{n}{random\PYGZus{}state} \PYG{o}{=} \PYG{l+m+mi}{1993}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{RES1}\PYG{p}{,}\PYG{n}{RES3}\PYG{p}{,}\PYG{n}{COM}\PYG{p}{]}\PYG{p}{)}



\PYG{c+c1}{\PYGZsh{} \PYGZsh{}\PYGZsh{}\PYGZsh{} Use BRAILS to download street view images}

\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{n}{sys}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/Users/simcenter/Codes/SimCenter/BIM.AI}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k+kn}{from} \PYG{n+nn}{brails}\PYG{n+nn}{.}\PYG{n+nn}{workflow}\PYG{n+nn}{.}\PYG{n+nn}{Images} \PYG{k+kn}{import} \PYG{n}{getGoogleImagesByAddrOrCoord}

\PYG{n}{addrs} \PYG{o}{=}  \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Longitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Latitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{to\PYGZus{}numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{getGoogleImagesByAddrOrCoord}\PYG{p}{(}\PYG{n}{Addrs}\PYG{o}{=}\PYG{n}{addrs}\PYG{p}{,} \PYG{n}{GoogleMapAPIKey}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Your\PYGZhy{}Key}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                             \PYG{n}{imageTypes}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StreetView}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}\PYG{n}{imgDir}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tmp/images}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{ncpu}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,}
                             \PYG{n}{fov}\PYG{o}{=}\PYG{l+m+mi}{60}\PYG{p}{,}\PYG{n}{pitch}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{reDownloadImgs}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}


\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StreetViewImg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{=}\PYG{n}{data}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{row}\PYG{p}{:} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tmp/images/StreetView/StreetViewx}\PYG{l+s+si}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}.6f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{\PYGZpc{}}\PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Longitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{x}\PYG{l+s+si}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}.6f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{\PYGZpc{}}\PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Latitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{.png}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{shutil}
\PYG{c+c1}{\PYGZsh{} Remove empty images}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StreetViewImg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{getsize}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{1024} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{]}
\PYG{c+c1}{\PYGZsh{} Remove duplicates}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StreetViewImg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} \PYGZsh{}\PYGZsh{}\PYGZsh{} Predict}

\PYG{k+kn}{from} \PYG{n+nn}{brails}\PYG{n+nn}{.}\PYG{n+nn}{modules} \PYG{k+kn}{import} \PYG{n}{OccupancyClassifier}
\PYG{n}{occupancyModel} \PYG{o}{=} \PYG{n}{OccupancyClassifier}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{occupancyPreds} \PYG{o}{=} \PYG{n}{occupancyModel}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StreetViewImg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}


\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Occupancy(BRAILS)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{=}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{occupancyPreds}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{prediction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{prob\PYGZus{}Occupancy(BRAILS)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{=}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{occupancyPreds}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{probability}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} \PYGZsh{}\PYGZsh{}\PYGZsh{} Plot confusion matrix}

\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{get\PYGZus{}ipython}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{run\PYGZus{}line\PYGZus{}magic}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{matplotlib}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{inline}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{sys}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k+kn}{from} \PYG{n+nn}{plotUtils} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{confusion\PYGZus{}matrix}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{f1\PYGZus{}score}\PYG{p}{,}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{,}\PYG{n}{f1\PYGZus{}score}

\PYG{n}{class\PYGZus{}names} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Occupancy(BRAILS)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{predictions} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Occupancy(BRAILS)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{labels} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{occupancy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{cnf\PYGZus{}matrix} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{predictions}\PYG{p}{,}\PYG{n}{labels}\PYG{o}{=}\PYG{n}{class\PYGZus{}names}\PYG{p}{)}
\PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{,} \PYG{n}{classes}\PYG{o}{=}\PYG{n}{class\PYGZus{}names}\PYG{p}{,} \PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{xlabel}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BRAILS}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NJDEP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}


\PYG{k}{for} \PYG{n}{i}\PYG{p}{,}\PYG{n}{cname} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{class\PYGZus{}names}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{accuracy} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}.1f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{\PYGZpc{}}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{/}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{TP} \PYG{o}{=} \PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}
    \PYG{n}{FP} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{p}{]}
    \PYG{n}{FN} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{p}{]}
    \PYG{n}{F1} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}.1f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{\PYGZpc{}}\PYG{p}{(}\PYG{n}{TP}\PYG{o}{/}\PYG{p}{(}\PYG{n}{TP}\PYG{o}{+}\PYG{l+m+mf}{0.5}\PYG{o}{*}\PYG{p}{(}\PYG{n}{FP}\PYG{o}{+}\PYG{n}{FN}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{cname}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{: Accuracy = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{accuracy}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, F1 = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{F1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} \PYGZsh{}\PYGZsh{}\PYGZsh{} Copy images to directories \PYGZob{}label\PYGZcb{}\PYGZhy{}\PYGZob{}prediction\PYGZcb{} for inspection}

\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{shutil}

\PYG{n}{predDir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tmp/images/occupancy\PYGZus{}predictions}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{n}{predDir}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{makedirs}\PYG{p}{(}\PYG{n}{predDir}\PYG{p}{)}

\PYG{n}{falseNames} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{def} \PYG{n+nf}{copyfiles}\PYG{p}{(}\PYG{n}{bim}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{for} \PYG{n}{ind}\PYG{p}{,} \PYG{n}{row} \PYG{o+ow}{in} \PYG{n}{bim}\PYG{o}{.}\PYG{n}{iterrows}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{label} \PYG{o}{=} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{occupancy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
        \PYG{n}{pred} \PYG{o}{=} \PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Occupancy(BRAILS)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

        \PYG{n}{lon}\PYG{p}{,} \PYG{n}{lat} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}.6f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{\PYGZpc{}}\PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Longitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}.6f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{\PYGZpc{}}\PYG{n}{row}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Latitude}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

        \PYG{n}{oldfile} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tmp/images/StreetView/StreetViewx}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{lon}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{x}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{lat}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{.png}\PYG{l+s+s1}{\PYGZsq{}}
        \PYG{n}{newfile} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{predDir}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{/}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{label}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{pred}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{/StreetViewx}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{lon}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{x}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{lat}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{.png}\PYG{l+s+s1}{\PYGZsq{}}

        \PYG{n}{thisFileDir} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{predDir}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{/}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{label}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{pred}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{/}\PYG{l+s+s1}{\PYGZsq{}}
        \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{n}{thisFileDir}\PYG{p}{)}\PYG{p}{:} \PYG{n}{os}\PYG{o}{.}\PYG{n}{makedirs}\PYG{p}{(}\PYG{n}{thisFileDir}\PYG{p}{)}

        \PYG{k}{try}\PYG{p}{:}
            \PYG{n}{shutil}\PYG{o}{.}\PYG{n}{copyfile}\PYG{p}{(}\PYG{n}{oldfile}\PYG{p}{,} \PYG{n}{newfile}\PYG{p}{)}
        \PYG{k}{except}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{oldfile}\PYG{p}{)}

\PYG{n}{copyfiles}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
In the files you downloaded, there are folders with names like RES\sphinxhyphen{}COM, which means those are images that are labelled as ‘RES’ in NJDEP dataset,
but they are predicted as ‘COM’. You can browse through those images to investigate deeper.

\sphinxAtStartPar
The confusion matrix tested on this dataset is shown in \hyperref[\detokenize{common/technical_manual/occupancy:fig-confusion-occupancy-njdep-v2}]{Fig.\@ \ref{\detokenize{common/technical_manual/occupancy:fig-confusion-occupancy-njdep-v2}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=0.400\linewidth]{{fig_confusion_occupancy_njdep_v2}.png}
\caption{Confusion matrix \sphinxhyphen{} Occupancy type classification for NJDEP}\label{\detokenize{common/technical_manual/occupancy:id3}}\label{\detokenize{common/technical_manual/occupancy:fig-confusion-occupancy-njdep-v2}}\end{figure}

\sphinxAtStartPar
The accuracy for the two classes are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
RES1: Accuracy = 0.89, F1 = 0.86

\item {} 
\sphinxAtStartPar
RES3: Accuracy = 0.92, F1 = 0.83

\item {} 
\sphinxAtStartPar
COM: Accuracy = 0.67, F1 = 0.79

\end{itemize}

\sphinxAtStartPar
Examples of false predictions are shown in \hyperref[\detokenize{common/technical_manual/occupancy:atlantic-occupancy-examples-njdep-falsev2}]{Table \ref{\detokenize{common/technical_manual/occupancy:atlantic-occupancy-examples-njdep-falsev2}}}.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Example of false predictions}\label{\detokenize{common/technical_manual/occupancy:id4}}\label{\detokenize{common/technical_manual/occupancy:atlantic-occupancy-examples-njdep-falsev2}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{StreetViewx-74.366315x39.422974}.png}
\sphinxfigcaption{Label: RES1, BRAILS Prediction: RES3}\label{\detokenize{common/technical_manual/occupancy:id5}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{StreetViewx-74.366873x39.420778}.png}
\sphinxfigcaption{Label: RES1, BRAILS Prediction: RES3}\label{\detokenize{common/technical_manual/occupancy:id6}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{StreetViewx-74.417573x39.372665}.png}
\sphinxfigcaption{Label: RES3, BRAILS Prediction: COM}\label{\detokenize{common/technical_manual/occupancy:id7}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{StreetViewx-74.418175x39.369580}.png}
\sphinxfigcaption{Label: RES3, BRAILS Prediction: COM}\label{\detokenize{common/technical_manual/occupancy:id8}}\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Bias in dataset is very common.
This validation doesn’t consider the possible bias in the labels (examples can be found in \hyperref[\detokenize{common/technical_manual/occupancy:njdep-occupancy-examples-biasv2}]{Table \ref{\detokenize{common/technical_manual/occupancy:njdep-occupancy-examples-biasv2}}}), which also negatively influences the accuracy.
\end{sphinxadmonition}


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Example of street view images: Bias in the labels}\label{\detokenize{common/technical_manual/occupancy:id9}}\label{\detokenize{common/technical_manual/occupancy:njdep-occupancy-examples-biasv2}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{StreetViewx-74.544719x39.459546}.png}
\sphinxfigcaption{Label: RES1, BRAILS Prediction: COM}\label{\detokenize{common/technical_manual/occupancy:id10}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{StreetViewx-74.358387x39.411702}.png}
\sphinxfigcaption{Label: RES1, BRAILS Prediction: RES3}\label{\detokenize{common/technical_manual/occupancy:id11}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{StreetViewx-74.412689x39.368096}.png}
\sphinxfigcaption{Label: RES3, BRAILS Prediction: COM}\label{\detokenize{common/technical_manual/occupancy:id12}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{StreetViewx-74.406136x39.382882}.png}
\sphinxfigcaption{Label: RES3, BRAILS Prediction: RES1}\label{\detokenize{common/technical_manual/occupancy:id13}}\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{Soft\sphinxhyphen{}story Building Classifier}
\label{\detokenize{common/technical_manual/softstory:soft-story-building-classifier}}\label{\detokenize{common/technical_manual/softstory:lbl-softstoryclassifier-vnv}}\label{\detokenize{common/technical_manual/softstory::doc}}
\sphinxAtStartPar
The Soft\sphinxhyphen{}story Building Classifier is validated here.

\sphinxAtStartPar
The trained classifier is tested on a ground truth dataset that can be downloaded \sphinxhref{https://zenodo.org/record/4508433/files/softstory-buildings-val.zip}{here}.
Accuracy is 83.8\%. Precision is 83.8\%. Recall is 83.8\%. F1 is 83.8\%.

\sphinxAtStartPar
Run the following python script to test on this dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} download the testing dataset}

\PYG{k+kn}{import} \PYG{n+nn}{wget}
\PYG{k+kn}{import} \PYG{n+nn}{zipfile}
\PYG{n}{wget}\PYG{o}{.}\PYG{n}{download}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://zenodo.org/record/4508433/files/softstory\PYGZhy{}buildings\PYGZhy{}val.zip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{with} \PYG{n}{zipfile}\PYG{o}{.}\PYG{n}{ZipFile}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{softstory\PYGZhy{}buildings\PYGZhy{}val.zip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{zip\PYGZus{}ref}\PYG{p}{:}
    \PYG{n}{zip\PYGZus{}ref}\PYG{o}{.}\PYG{n}{extractall}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} prepare the image lists}

\PYG{k+kn}{import} \PYG{n+nn}{shutil}
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{from} \PYG{n+nn}{glob} \PYG{k+kn}{import} \PYG{n}{glob}

\PYG{n}{softstoryList} \PYG{o}{=} \PYG{n}{glob}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{softstory\PYGZhy{}buildings\PYGZhy{}val/softstory/*.png}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{othersList} \PYG{o}{=} \PYG{n}{glob}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{softstory\PYGZhy{}buildings\PYGZhy{}val/others/*.png}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} define the paths of images in a list}
\PYG{n}{imgs}\PYG{o}{=}\PYG{n}{softstoryList}\PYG{o}{+}\PYG{n}{othersList}

\PYG{c+c1}{\PYGZsh{} import the module}
\PYG{k+kn}{from} \PYG{n+nn}{brails}\PYG{n+nn}{.}\PYG{n+nn}{modules} \PYG{k+kn}{import} \PYG{n}{SoftstoryClassifier}

\PYG{c+c1}{\PYGZsh{} initialize the classifier}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{SoftstoryClassifier}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} use the model to predict}
\PYG{n}{predictions} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{imgs}\PYG{p}{)}

\PYG{n}{prediction} \PYG{o}{=} \PYG{n}{predictions}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{prediction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{label} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{softstory}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{*}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{softstoryList}\PYG{p}{)} \PYG{o}{+} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{others}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{*}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{othersList}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot results}
\PYG{n}{class\PYGZus{}names} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{softstory}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{others}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{k+kn}{from} \PYG{n+nn}{brails}\PYG{n+nn}{.}\PYG{n+nn}{utils}\PYG{n+nn}{.}\PYG{n+nn}{plotUtils} \PYG{k+kn}{import} \PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{confusion\PYGZus{}matrix}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{f1\PYGZus{}score}\PYG{p}{,}\PYG{n}{accuracy\PYGZus{}score}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ Accuracy is   : }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{, Random guess is 0.5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{prediction}\PYG{p}{,}\PYG{n}{label}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{cnf\PYGZus{}matrix} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{prediction}\PYG{p}{,}\PYG{n}{label}\PYG{p}{)}
\PYG{n}{plot\PYGZus{}confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{cnf\PYGZus{}matrix}\PYG{p}{,} \PYG{n}{classes}\PYG{o}{=}\PYG{n}{class\PYGZus{}names}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Confusion matrix}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}\PYG{n}{xlabel}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Labels}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{ylabel}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predictions}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
The confusion matrix tested on this dataset is shown in \hyperref[\detokenize{common/technical_manual/softstory:fig-confusion-softstory}]{Fig.\@ \ref{\detokenize{common/technical_manual/softstory:fig-confusion-softstory}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=0.400\linewidth]{{confusion_softstory}.png}
\caption{Confusion matrix \sphinxhyphen{} Soft\sphinxhyphen{}story building classifier}\label{\detokenize{common/technical_manual/softstory:id1}}\label{\detokenize{common/technical_manual/softstory:fig-confusion-softstory}}\end{figure}


\subsection{Number of Floors Detector}
\label{\detokenize{common/technical_manual/nfloor:number-of-floors-detector}}\label{\detokenize{common/technical_manual/nfloor:lbl-nfloordetector-vnv}}\label{\detokenize{common/technical_manual/nfloor::doc}}
\sphinxAtStartPar
On a randomly selected set of in\sphinxhyphen{}the\sphinxhyphen{}wild building images from New Jersey’s Bergen, Middlesex, and Moris Counties, the model attains an F1\sphinxhyphen{}score of 86\%. Here, in\sphinxhyphen{}the\sphinxhyphen{}wild building images are defined as street\sphinxhyphen{}level photos that may contain multiple buildings and are captured with random camera properties. \sphinxcode{\sphinxupquote{confusion\_nFloorWildv2}} is the confusion matrix of the model inferences on the aforementioned in\sphinxhyphen{}the\sphinxhyphen{}wild test set.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=0.700\linewidth]{{confusion_nFloorWild}.png}
\caption{Confusion matrix of the pretrained model on the in\sphinxhyphen{}the\sphinxhyphen{}wild test set}\label{\detokenize{common/technical_manual/nfloor:id1}}\label{\detokenize{common/technical_manual/nfloor:confusion-nfloorwild}}\end{figure}

\sphinxAtStartPar
If the test images are constrained such that a single building exists in each image, the building is viewed with minimal obstructions, and the images are captured such that the image plane is nearly parallel to the frontal plane of the building facade, the F1\sphinxhyphen{}score of the model is determined as 94.7\%. \sphinxcode{\sphinxupquote{confusion\_nFloorClean}} shows the confusion matrix for the pretrained model on a test set generated according to these constraints.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=0.700\linewidth]{{confusion_nFloorClean}.png}
\caption{Confusion matrix of the pretrained model on the dataset containing lightly distorted/obstructed images of individual buildings}\label{\detokenize{common/technical_manual/nfloor:id2}}\label{\detokenize{common/technical_manual/nfloor:confusion-nfloorcleanv2}}\end{figure}

\sphinxAtStartPar
\hyperref[\detokenize{common/technical_manual/nfloor:inthewild-removed}]{Table \ref{\detokenize{common/technical_manual/nfloor:inthewild-removed}}} shows a sample of images  removed from the in\sphinxhyphen{}the\sphinxhyphen{}wild test set that were found to display weak resemblance of the visual cues necessary for a valid number of floor predictions.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{In\sphinxhyphen{}the\sphinxhyphen{}wild street level imagery removed as a part of dataset cleaning}\label{\detokenize{common/technical_manual/nfloor:id3}}\label{\detokenize{common/technical_manual/nfloor:inthewild-removed}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{occluded1}.jpg}
\sphinxfigcaption{Heavily occluded building facade}\label{\detokenize{common/technical_manual/nfloor:id4}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{multipleBuildings}.jpg}
\sphinxfigcaption{Closely spaced buildings: obscure prediction target}\label{\detokenize{common/technical_manual/nfloor:id5}}\end{sphinxfigure-in-table}\relax
\\
\hline\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{persDistort}.jpg}
\sphinxfigcaption{Significant perspective distortions}\label{\detokenize{common/technical_manual/nfloor:id6}}\end{sphinxfigure-in-table}\relax
&\begin{sphinxfigure-in-table}
\centering
\capstart
\noindent\sphinxincludegraphics{{occluded2}.jpg}
\sphinxfigcaption{Heavily occluded building facade}\label{\detokenize{common/technical_manual/nfloor:id7}}\end{sphinxfigure-in-table}\relax
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\begin{sphinxadmonition}{note}{Note:}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Year Built Classifier is currently under active development and testing. More details about the training, modification, improvement, and validation of this module can be found \sphinxhref{https://github.com/NHERI-SimCenter/BRAILS/tree/master/brails/modules/Year\_Built\_Classifier}{here}.

\item {} 
\sphinxAtStartPar
Raised Foundation Classifier is currently under active development and testing. More details about the training, modification, improvement, and validation of this module can be found \sphinxhref{https://github.com/NHERI-SimCenter/BRAILS/tree/master/brails/modules/Foundation\_Classification}{here}.

\end{enumerate}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
DISCLAIMER:
The modules are implemented to demonstrate the potentials of ML methods to help establish building attributes and inventories for regional scale simulation.
The modules are tested extensively using the data sets as reported herein for validations.
How these modules generalize to new and unseen data from different geographical locations depends on how similar they are to the training data.
Generalization of machine learning models remains an active research area.
Users should exercise cautions when the modules are used beyond their intended purposes and trained model ability.
\end{sphinxadmonition}
\phantomsection\label{\detokenize{index:lbl-developer-manual}}

\section{How to Extend}
\label{\detokenize{common/developer_manual/how_to_extend/how_to_extend:how-to-extend}}\label{\detokenize{common/developer_manual/how_to_extend/how_to_extend:lbl-how-to-extend}}\label{\detokenize{common/developer_manual/how_to_extend/how_to_extend::doc}}
\sphinxAtStartPar
The framework is being developed and maintained by the author.

\sphinxAtStartPar
The developer manual will be provided here if community contributions are needed in the future.


\section{Coding Style}
\label{\detokenize{common/developer_manual/coding_style/coding_style:coding-style}}\label{\detokenize{common/developer_manual/coding_style/coding_style:lblcodingstyle}}\label{\detokenize{common/developer_manual/coding_style/coding_style::doc}}

\subsection{Python Style}
\label{\detokenize{common/developer_manual/coding_style/coding_style:python-style}}
\sphinxAtStartPar
For code written in Python SimCenter programmers follow the widely used
\sphinxhref{https://www.python.org/dev/peps/pep-0008/}{Guide PEP 8}


\chapter{How to cite}
\label{\detokenize{index:how-to-cite}}
\sphinxAtStartPar
Charles Wang, Sascha Hornauer, Barbaros Cetiner, Yunhui Guo, Frank McKenna, Qian Yu, Stella X. Yu, Ertugrul Taciroglu, \& Kincho H. Law. (2021, March 1). NHERI\sphinxhyphen{}SimCenter/BRAILS: Release v2.0.0 (Version v2.0.0). Zenodo. \sphinxurl{http://doi.org/10.5281/zenodo.4570554}


\chapter{Contact}
\label{\detokenize{index:contact}}
\sphinxAtStartPar
NHERI\sphinxhyphen{}SimCenter \sphinxhref{mailto:nheri-simcenter@berkeley.edu}{nheri\sphinxhyphen{}simcenter@berkeley.edu}


\chapter{References}
\label{\detokenize{index:references}}
\sphinxAtStartPar


\begin{sphinxthebibliography}{BHF+19}
\bibitem[BHF+19]{common/technical_manual/framework:bischke2019multi}
\sphinxAtStartPar
Benjamin Bischke, Patrick Helber, Joachim Folz, Damian Borth, and Andreas Dengel. Multi\sphinxhyphen{}task learning for segmentation of building footprints with deep neural networks. \sphinxstyleemphasis{2019 IEEE International Conference on Image Processing (ICIP)}, pages 1480\textendash{}1484, 2019.
\bibitem[Goo97]{common/technical_manual/framework:goovaerts1997geostatistics}
\sphinxAtStartPar
Pierre Goovaerts. \sphinxstyleemphasis{Geostatistics for natural resources evaluation}. Oxford University Press on Demand, 1997.
\bibitem[HW68]{common/technical_manual/framework:hubel1968receptive}
\sphinxAtStartPar
David H Hubel and Torsten N Wiesel. Receptive fields and functional architecture of monkey striate cortex. \sphinxstyleemphasis{The Journal of physiology}, 195(1):215\textendash{}243, 1968.
\bibitem[LHF+19]{common/technical_manual/framework:li2019semantic}
\sphinxAtStartPar
Weijia Li, Conghui He, Jiarui Fang, Juepeng Zheng, Haohuan Fu, and Le Yu. Semantic segmentation\sphinxhyphen{}based building footprint extraction using very high\sphinxhyphen{}resolution satellite images and multi\sphinxhyphen{}source gis data. \sphinxstyleemphasis{Remote Sensing}, 11(4):403, 2019.
\bibitem[Mic]{common/technical_manual/framework:msfootprint}
\sphinxAtStartPar
Microsoft. US Building Footprints. URL: \sphinxurl{https://github.com/microsoft/USBuildingFootprints}.
\bibitem[Van10]{common/technical_manual/framework:vanmarcke2010random}
\sphinxAtStartPar
Erik Vanmarcke. \sphinxstyleemphasis{Random fields: analysis and synthesis}. World Scientific, 2010.
\bibitem[WC18]{common/technical_manual/framework:wang2017hybrid}
\sphinxAtStartPar
C Wang and Q Chen. A hybrid geotechnical and geological data\sphinxhyphen{}based framework for multiscale regional liquefaction hazard mapping. \sphinxstyleemphasis{Géotechnique}, 68(7):614\textendash{}625, 2018.
\bibitem[WCSJ17]{common/technical_manual/framework:wang2017spatial}
\sphinxAtStartPar
Chaofeng Wang, Qiushi Chen, Mengfen Shen, and C Hsein Juang. On the spatial variability of cpt\sphinxhyphen{}based geotechnical parameters for regional liquefaction evaluation. \sphinxstyleemphasis{Soil Dynamics and Earthquake Engineering}, 95:153\textendash{}166, 2017.
\bibitem[ZKJS18]{common/technical_manual/framework:zhao2018building}
\sphinxAtStartPar
Kang Zhao, Jungwon Kang, Jaewook Jung, and Gunho Sohn. Building extraction from satellite images using mask r\sphinxhyphen{}cnn with building boundary regularization. \sphinxstyleemphasis{CVPR Workshops}, pages 247\textendash{}251, 2018.
\bibitem[BHF+19]{index:bischke2019multi}
\sphinxAtStartPar
Benjamin Bischke, Patrick Helber, Joachim Folz, Damian Borth, and Andreas Dengel. Multi\sphinxhyphen{}task learning for segmentation of building footprints with deep neural networks. \sphinxstyleemphasis{2019 IEEE International Conference on Image Processing (ICIP)}, pages 1480\textendash{}1484, 2019.
\bibitem[Goo97]{index:goovaerts1997geostatistics}
\sphinxAtStartPar
Pierre Goovaerts. \sphinxstyleemphasis{Geostatistics for natural resources evaluation}. Oxford University Press on Demand, 1997.
\bibitem[HZRS16]{index:he2016deep}
\sphinxAtStartPar
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In \sphinxstyleemphasis{Proceedings of the IEEE conference on computer vision and pattern recognition}, 770\textendash{}778. 2016.
\bibitem[HW68]{index:hubel1968receptive}
\sphinxAtStartPar
David H Hubel and Torsten N Wiesel. Receptive fields and functional architecture of monkey striate cortex. \sphinxstyleemphasis{The Journal of physiology}, 195(1):215\textendash{}243, 1968.
\bibitem[LHF+19]{index:li2019semantic}
\sphinxAtStartPar
Weijia Li, Conghui He, Jiarui Fang, Juepeng Zheng, Haohuan Fu, and Le Yu. Semantic segmentation\sphinxhyphen{}based building footprint extraction using very high\sphinxhyphen{}resolution satellite images and multi\sphinxhyphen{}source gis data. \sphinxstyleemphasis{Remote Sensing}, 11(4):403, 2019.
\bibitem[Mic]{index:msfootprint}
\sphinxAtStartPar
Microsoft. US Building Footprints. URL: \sphinxurl{https://github.com/microsoft/USBuildingFootprints}.
\bibitem[SVI+16]{index:szegedy2016rethinking}
\sphinxAtStartPar
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. \sphinxstyleemphasis{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 2818\textendash{}2826, 2016.
\bibitem[Van10]{index:vanmarcke2010random}
\sphinxAtStartPar
Erik Vanmarcke. \sphinxstyleemphasis{Random fields: analysis and synthesis}. World Scientific, 2010.
\bibitem[WC18]{index:wang2017hybrid}
\sphinxAtStartPar
C Wang and Q Chen. A hybrid geotechnical and geological data\sphinxhyphen{}based framework for multiscale regional liquefaction hazard mapping. \sphinxstyleemphasis{Géotechnique}, 68(7):614\textendash{}625, 2018.
\bibitem[Wan20]{index:chaofeng-wang-2020-4386991}
\sphinxAtStartPar
Chaofeng Wang. Occupancy test. December 2020. URL: \sphinxurl{https://doi.org/10.5281/zenodo.4386991}, \sphinxhref{https://doi.org/10.5281/zenodo.4386991}{doi:10.5281/zenodo.4386991}.
\bibitem[WCSJ17]{index:wang2017spatial}
\sphinxAtStartPar
Chaofeng Wang, Qiushi Chen, Mengfen Shen, and C Hsein Juang. On the spatial variability of cpt\sphinxhyphen{}based geotechnical parameters for regional liquefaction evaluation. \sphinxstyleemphasis{Soil Dynamics and Earthquake Engineering}, 95:153\textendash{}166, 2017.
\bibitem[Wan19]{index:charles-wang-2019-3521067}
\sphinxAtStartPar
Charles Wang. Random satellite images of buildings. October 2019. URL: \sphinxurl{https://doi.org/10.5281/zenodo.3521067}, \sphinxhref{https://doi.org/10.5281/zenodo.3521067}{doi:10.5281/zenodo.3521067}.
\bibitem[WYM+19]{index:brails}
\sphinxAtStartPar
Charles Wang, Qian Yu, Frank McKenna, Barbaros Cetiner, Stella X. Yu, Ertugrul Taciroglu, and Kincho H. Law. Nheri\sphinxhyphen{}simcenter/brails: v1.0.1. October 2019. URL: \sphinxurl{https://doi.org/10.5281/zenodo.3483208}, \sphinxhref{https://doi.org/10.5281/zenodo.3483208}{doi:10.5281/zenodo.3483208}.
\bibitem[ZKJS18]{index:zhao2018building}
\sphinxAtStartPar
Kang Zhao, Jungwon Kang, Jaewook Jung, and Gunho Sohn. Building extraction from satellite images using mask r\sphinxhyphen{}cnn with building boundary regularization. \sphinxstyleemphasis{CVPR Workshops}, pages 247\textendash{}251, 2018.
\end{sphinxthebibliography}



\renewcommand{\indexname}{Index}
\printindex
\end{document}